"""
åŸºäºäº‹ä»¶è§¦å‘åˆ†å±‚å®‰å…¨å¼ºåŒ–å­¦ä¹ çš„æœºå™¨äººå¯¼èˆªè®­ç»ƒä¸»ç¨‹åº
ä½¿ç”¨ETHSRL(Event-Triggered Hierarchical Safe Reinforcement Learning)æ¡†æ¶è¿›è¡Œæœºå™¨äººå¯¼èˆª
"""

from myRL import ETHSRLAgent  # ä¿®æ”¹äº†å¯¼å…¥è·¯å¾„
import torch
import numpy as np
import time
import os
from datetime import datetime

from robot_nav.SIM_ENV.sim import SIM  # å¯¼å…¥ä»¿çœŸç¯å¢ƒ
from utils.buffer import HierarchicalReplayBuffer  # ä¿®æ”¹äº†å¯¼å…¥è·¯å¾„

# è®­ç»ƒé…ç½®è¾“å‡º
print("=== ETHSRL Training Configuration ===")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDA version: {torch.version.cuda}")
else:
    print("Training on CPU")
print("==================================")

def main(args=None):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # ========== è®­ç»ƒå‚æ•°é…ç½® ==========
    action_dim = 2  # æ¨¡å‹è¾“å‡ºçš„åŠ¨ä½œç»´åº¦ [çº¿é€Ÿåº¦, è§’é€Ÿåº¦]
    max_action = 1  # è¾“å‡ºåŠ¨ä½œçš„æœ€å¤§ç»å¯¹å€¼
    state_dim = 95  # ç¥ç»ç½‘ç»œè¾“å…¥çŠ¶æ€çš„ç»´åº¦ï¼ˆçŠ¶æ€å‘é‡çš„é•¿åº¦ï¼‰
    device = torch.device(
        "cuda" if torch.cuda.is_available() else "cpu"
    )  # è®¾å¤‡é€‰æ‹©ï¼šå¦‚æœå¯ç”¨åˆ™ä½¿ç”¨CUDAï¼Œå¦åˆ™ä½¿ç”¨CPU
    nr_eval_episodes = 10  # è¯„ä¼°æ—¶ä½¿ç”¨çš„å›åˆæ•°
    max_epochs = 60  # æœ€å¤§è®­ç»ƒè½®æ•°
    epoch = 0  # èµ·å§‹è½®æ•°
    episodes_per_epoch = 70  # æ¯ä¸ªè®­ç»ƒè½®ä¸­è¿è¡Œçš„å›åˆæ•°
    episode = 0  # èµ·å§‹å›åˆæ•°
    train_every_n = 2  # æ¯nä¸ªå›åˆè®­ç»ƒå’Œæ›´æ–°ä¸€æ¬¡ç½‘ç»œå‚æ•°
    training_iterations = 80  # å•æ¬¡è®­ç»ƒå‘¨æœŸä¸­ä½¿ç”¨çš„æ‰¹æ¬¡æ•°
    batch_size = 64  # æ¯æ¬¡è®­ç»ƒè¿­ä»£çš„æ‰¹æ¬¡å¤§å°
    max_steps = 300  # å•ä¸ªå›åˆä¸­çš„æœ€å¤§æ­¥æ•°
    steps = 0  # èµ·å§‹æ­¥æ•°
    load_saved_buffer = False  # æ˜¯å¦ä»assets/data.ymlåŠ è½½ç»éªŒæ•°æ®
    pretrain = False  # æ˜¯å¦ä½¿ç”¨åŠ è½½çš„ç»éªŒæ•°æ®é¢„è®­ç»ƒæ¨¡å‹ï¼ˆload_saved_bufferå¿…é¡»ä¸ºTrueï¼‰
    pretraining_iterations = 10  # é¢„è®­ç»ƒæœŸé—´è¿è¡Œçš„è®­ç»ƒè¿­ä»£æ¬¡æ•°
    save_every = 5  # æ¯nä¸ªè®­ç»ƒå‘¨æœŸä¿å­˜ä¸€æ¬¡æ¨¡å‹
    history_length = 8  # å†å²è§‚æµ‹é•¿åº¦ï¼Œç”¨äºPOMDPä¿¡å¿µçŠ¶æ€
    world_file = "worlds/env_a.yaml"  # ä»¿çœŸç¯å¢ƒä¸–ç•Œæ–‡ä»¶
    save_dir = "saved_models/ETHSRL"  # æ¨¡å‹ä¿å­˜ç›®å½•
    
    # ETHSRLç‰¹å®šå‚æ•°
    risk_threshold = 0.7  # é£é™©è§¦å‘é˜ˆå€¼
    proximity_threshold = 0.5  # éšœç¢ç‰©æ¥è¿‘é˜ˆå€¼(ç±³)
    heading_threshold = 0.3  # èˆªå‘å˜åŒ–é˜ˆå€¼(å¼§åº¦)
    min_trigger_interval = 1.0  # æœ€å°è§¦å‘é—´éš”(ç§’)
    k_d = 1.0  # Lyapunovè·ç¦»å¢ç›Š
    k_theta = 1.0  # Lyapunovè§’åº¦å¢ç›Š
    v_threshold = 0.5  # Lyapunové˜ˆå€¼

    # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)

    # ========== è®­ç»ƒåˆå§‹åŒ–æ—¥å¿— ==========
    print("\n" + "="*60)
    print("ğŸš€ Starting ETHSRL Navigation Training")
    print("="*60)
    print(f"ğŸ“‹ Training Configuration:")
    print(f"   â€¢ Device: {device}")
    print(f"   â€¢ State dim: {state_dim}, Action dim: {action_dim}")
    print(f"   â€¢ Max epochs: {max_epochs}, Episodes per epoch: {episodes_per_epoch}")
    print(f"   â€¢ Training iterations: {training_iterations}, Batch size: {batch_size}")
    print(f"   â€¢ History length: {history_length}")
    print(f"   â€¢ World file: {world_file}")
    print(f"   â€¢ Risk threshold: {risk_threshold}")
    print(f"   â€¢ Min trigger interval: {min_trigger_interval}s")
    print(f"   â€¢ Lyapunov parameters: k_d={k_d}, k_theta={k_theta}, V_threshold={v_threshold}")
    print("="*60)

    # ========== æ¨¡å‹åˆå§‹åŒ– ==========
    # åˆ›å»ºé…ç½®å­—å…¸
    config = {
        'RISK_THRESHOLD': risk_threshold,
        'PROXIMITY_THRESHOLD': proximity_threshold,
        'HEADING_THRESHOLD': heading_threshold,
        'MIN_TRIGGER_INTERVAL': min_trigger_interval,
        'K_D': k_d,
        'K_THETA': k_theta,
        'V_THRESHOLD': v_threshold,
        'HISTORY_LENGTH': history_length,
        'TOTAL_EPOCHS': max_epochs,
        'BATCH_SIZE': batch_size,
        'TRAIN_EVERY_N': train_every_n
    }
    
    model = ETHSRLAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        config=config,
        device=device
    )

    # ========== ç¯å¢ƒåˆå§‹åŒ– ==========
    sim = SIM(
        world_file=world_file, disable_plotting=False
    )  # å®ä¾‹åŒ–ä»¿çœŸç¯å¢ƒï¼Œå¯ç”¨å›¾å½¢æ˜¾ç¤º

    # ========== ç»éªŒå›æ”¾ç¼“å†²åŒºåˆå§‹åŒ– ==========
    # ä¿®æ”¹ä¸ºç›´æ¥ä½¿ç”¨HierarchicalReplayBuffer
    replay_buffer = HierarchicalReplayBuffer(
        easy_capacity=10000,
        medium_capacity=20000, 
        hard_capacity=30000,
        alpha=0.6,
        beta=0.4
    )

    # ========== è®­ç»ƒç»Ÿè®¡å˜é‡åˆå§‹åŒ– ==========
    episode_reward = 0.0  # å½“å‰å›åˆç´¯è®¡å¥–åŠ±
    epoch_total_reward = 0.0  # å½“å‰epochç´¯è®¡å¥–åŠ±
    epoch_total_steps = 0  # å½“å‰epochç´¯è®¡æ­¥æ•°
    epoch_goal_count = 0  # å½“å‰epochæˆåŠŸåˆ°è¾¾ç›®æ ‡æ¬¡æ•°
    epoch_collision_count = 0  # å½“å‰epochç¢°æ’æ¬¡æ•°
    epoch_high_triggers = 0  # å½“å‰epoché«˜å±‚è§¦å‘æ¬¡æ•°
    epoch_safety_triggers = 0  # å½“å‰epochå®‰å…¨å±‚è§¦å‘æ¬¡æ•°
    
    # ========== åˆå§‹çŠ¶æ€è·å– ==========
    # æ‰§è¡Œåˆå§‹æ­¥è¿›ï¼Œè·å–ç¯å¢ƒåˆå§‹çŠ¶æ€ï¼ˆé›¶é€Ÿåº¦ï¼‰
    latest_scan, distance, cos, sin, collision, goal, a, reward = sim.step(
        lin_velocity=0.0, ang_velocity=0.0
    )

    # ========== ä¸»è®­ç»ƒå¾ªç¯ ==========
    training_start_time = time.time()  # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
    print("\nğŸ“Š Training Progress:")
    
    while epoch < max_epochs:  # è®­ç»ƒç›´åˆ°è¾¾åˆ°æœ€å¤§è½®æ•°
        epoch_start_time = time.time()  # è®°å½•å½“å‰epochå¼€å§‹æ—¶é—´
        
        # æ›´æ–°è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ
        model.update_curriculum(epoch)
        
        # é‡ç½®å½“å‰epochçš„ç»Ÿè®¡æ•°æ®
        epoch_total_reward = 0.0
        epoch_total_steps = 0
        epoch_goal_count = 0
        epoch_collision_count = 0
        epoch_high_triggers = 0
        epoch_safety_triggers = 0
        
        # æ¯è½®è®­ç»ƒæŒ‡å®šå›åˆæ•°
        while episode < episodes_per_epoch:
            # å‡†å¤‡å½“å‰çŠ¶æ€è¡¨ç¤º
            state, terminal = model.prepare_state(
                latest_scan,  # æœ€æ–°æ¿€å…‰é›·è¾¾æ‰«ææ•°æ®
                distance,     # åˆ°ç›®æ ‡çš„è·ç¦»
                cos,          # ç›®æ ‡æ–¹å‘ä½™å¼¦å€¼
                sin,          # ç›®æ ‡æ–¹å‘æ­£å¼¦å€¼
                collision,    # ç¢°æ’æ ‡å¿—
                goal,         # åˆ°è¾¾ç›®æ ‡æ ‡å¿—
                a             # ä¸Šä¸€ä¸ªåŠ¨ä½œ
            )
            
            # ä»æ¨¡å‹è·å–åŠ¨ä½œï¼ˆæ¢ç´¢æ¨¡å¼ï¼‰
            goal_pos = [distance * cos, distance * sin]  # ç›¸å¯¹ç›®æ ‡ä½ç½®
            action, info = model.select_action(state, goal_pos, scan_data=latest_scan)
            
            # åŠ¨ä½œåå¤„ç†ï¼šå°†çº¿æ€§é€Ÿåº¦ä»[-1,1]æ˜ å°„åˆ°[0,0.5]èŒƒå›´
            a_in = [
                (action[0] + 1) / 4,  # çº¿æ€§é€Ÿåº¦ï¼šæ˜ å°„åˆ°[0, 0.5] m/sèŒƒå›´
                action[1],            # è§’é€Ÿåº¦ï¼šä¿æŒåŸå€¼
            ]

            # åœ¨ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œï¼Œè·å–æ–°çŠ¶æ€å’Œå¥–åŠ±
            latest_scan, distance, cos, sin, collision, goal, a, reward = sim.step(
                lin_velocity=a_in[0],   # åº”ç”¨å¤„ç†åçš„çº¿é€Ÿåº¦
                ang_velocity=a_in[1]    # åº”ç”¨è§’é€Ÿåº¦
            )
            
            # å‡†å¤‡ä¸‹ä¸€ä¸ªçŠ¶æ€è¡¨ç¤º
            next_state, terminal = model.prepare_state(
                latest_scan, distance, cos, sin, collision, goal, a
            )
            
            # å­˜å‚¨ç»éªŒ
            model.store_experience(
                state, action, reward, terminal, next_state, info
            )
            
            # åŒæ—¶æ·»åŠ åˆ°æ ‡å‡†ç¼“å†²åŒºï¼ˆç”¨äºå…¼å®¹ï¼‰
            replay_buffer.add(
                state, action, next_state, reward, terminal
            )
            
            # æ›´æ–°ç»Ÿè®¡
            episode_reward += reward
            epoch_total_reward += reward
            steps += 1
            epoch_total_steps += 1
            
            # è®°å½•è§¦å‘äº‹ä»¶
            if info['high_triggered']:
                epoch_high_triggers += 1
            if info['safety_triggered']:
                epoch_safety_triggers += 1

            # æ£€æŸ¥å›åˆæ˜¯å¦ç»“æŸï¼ˆåˆ°è¾¾ç»ˆæ­¢çŠ¶æ€æˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼‰
            if terminal or steps == max_steps:
                # è®°å½•æˆåŠŸ/å¤±è´¥
                if goal:
                    epoch_goal_count += 1
                if collision:
                    epoch_collision_count += 1
                
                # æ‰“å°å›åˆä¿¡æ¯
                print(f"Episode {episode+1}/{episodes_per_epoch}: " + 
                      f"Reward={episode_reward:.2f}, Steps={steps}, " +
                      f"Result={'Success' if goal else 'Collision' if collision else 'Timeout'}, " +
                      f"High Triggers={model.high_trigger_count}, Safety Triggers={model.safety_trigger_count}")
                
                # é‡ç½®ç¯å¢ƒï¼Œè·å–æ–°çš„åˆå§‹çŠ¶æ€
                latest_scan, distance, cos, sin, collision, goal, a, reward = sim.reset()
                episode += 1  # å›åˆè®¡æ•°å¢åŠ 
                
                # å®šæœŸè®­ç»ƒæ¨¡å‹
                if episode % train_every_n == 0:
                    model.train(batch_size=batch_size)  # è®­ç»ƒæ¨¡å‹å¹¶æ›´æ–°å‚æ•°

                # é‡ç½®å›åˆç»Ÿè®¡
                episode_reward = 0.0
                steps = 0  # é‡ç½®æ­¥æ•°è®¡æ•°å™¨
                model.reset()  # é‡ç½®æ¨¡å‹çŠ¶æ€
            
        # è®¡ç®—epochç»Ÿè®¡
        success_rate = epoch_goal_count / episodes_per_epoch
        collision_rate = epoch_collision_count / episodes_per_epoch
        avg_reward = epoch_total_reward / episodes_per_epoch
        avg_steps = epoch_total_steps / episodes_per_epoch
        avg_high_triggers = epoch_high_triggers / episodes_per_epoch
        avg_safety_triggers = epoch_safety_triggers / episodes_per_epoch
        epoch_time = time.time() - epoch_start_time
        
        # æ‰“å°epochç»“æœ
        print(f"\n== Epoch {epoch+1}/{max_epochs} ç»Ÿè®¡ ==")
        print(f"ğŸ¯ æˆåŠŸç‡: {success_rate:.2f}, ç¢°æ’ç‡: {collision_rate:.2f}")
        print(f"ğŸ† å¹³å‡å¥–åŠ±: {avg_reward:.2f}, å¹³å‡æ­¥æ•°: {avg_steps:.2f}")
        print(f"âš¡ å¹³å‡é«˜å±‚è§¦å‘: {avg_high_triggers:.2f}, å¹³å‡å®‰å…¨è§¦å‘: {avg_safety_triggers:.2f}")
        print(f"â±ï¸ ç”¨æ—¶: {epoch_time:.2f}ç§’")
        print("="*40)
        
        # é‡ç½®å›åˆè®¡æ•°
        episode = 0
        epoch += 1  # è®­ç»ƒè½®æ•°å¢åŠ 
        
        # ä¿å­˜æ¨¡å‹
        if epoch % save_every == 0:
            timestamp = datetime.now().strftime("%Y%m%d-%H%M")
            save_path = f"{save_dir}/ethsrl_epoch_{epoch}_{timestamp}.pt"
            model.save(save_path)
            print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜: {save_path}")
        
        # è¯„ä¼°å½“å‰æ¨¡å‹
        if epoch % save_every == 0:
            evaluate(model, sim, nr_eval_episodes, epoch)

    # è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = f"{save_dir}/ethsrl_final.pt"
    model.save(final_path)
    print(f"ğŸ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_path}")
    
    # æ€»è®­ç»ƒæ—¶é—´
    total_training_time = time.time() - training_start_time
    hours = int(total_training_time // 3600)
    minutes = int((total_training_time % 3600) // 60)
    seconds = int(total_training_time % 60)
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {hours}h {minutes}m {seconds}s")
    
    # æœ€ç»ˆè¯„ä¼°
    print("\nğŸ§ª å¼€å§‹æœ€ç»ˆè¯„ä¼°...")
    evaluate(model, sim, nr_eval_episodes * 2, epoch)


def evaluate(model, sim, eval_episodes, epoch=None):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    å‚æ•°:
        model: è¦è¯„ä¼°çš„æ¨¡å‹
        sim: ä»¿çœŸç¯å¢ƒå®ä¾‹
        eval_episodes: è¯„ä¼°å›åˆæ•°
        epoch: å½“å‰è®­ç»ƒè½®æ•°
    """
    print("\n" + "="*50)
    print(f"ğŸ” å¼€å§‹è¯„ä¼° (å›åˆæ•°: {eval_episodes})")
    print("="*50)
    
    # åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡
    rewards = []
    steps_list = []
    success_count = 0
    collision_count = 0
    timeout_count = 0
    high_triggers_list = []
    safety_triggers_list = []
    
    for ep in range(eval_episodes):
        # é‡ç½®ç¯å¢ƒä¸æ¨¡å‹
        latest_scan, distance, cos, sin, collision, goal, a, _ = sim.reset()
        model.reset()
        
        # åˆå§‹åŒ–å›åˆç»Ÿè®¡
        episode_reward = 0
        steps = 0
        high_triggers_before = model.high_trigger_count
        safety_triggers_before = model.safety_trigger_count
        
        # å•å›åˆå¾ªç¯
        done = False
        while not done and steps < 500:  # è¯„ä¼°ç”¨æ›´é•¿çš„æ­¥æ•°é™åˆ¶
            # å‡†å¤‡å½“å‰çŠ¶æ€
            state, _ = model.prepare_state(latest_scan, distance, cos, sin, collision, goal, a)
            
            # é€‰æ‹©åŠ¨ä½œï¼ˆç¡®å®šæ€§æ¨¡å¼ï¼‰
            goal_pos = [distance * cos, distance * sin]
            action, _ = model.select_action(state, goal_pos, scan_data=latest_scan, deterministic=True)
            
            # åŠ¨ä½œæ˜ å°„
            a_in = [(action[0] + 1) / 4, action[1]]
            
            # æ‰§è¡ŒåŠ¨ä½œ
            latest_scan, distance, cos, sin, collision, goal, a, reward = sim.step(
                lin_velocity=a_in[0], ang_velocity=a_in[1]
            )
            
            # æ›´æ–°ç»Ÿè®¡
            episode_reward += reward
            steps += 1
            
            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            done = collision or goal
        
        # è®¡ç®—æ­¤å›åˆè§¦å‘æ¬¡æ•°
        high_triggers = model.high_trigger_count - high_triggers_before
        safety_triggers = model.safety_trigger_count - safety_triggers_before
        
        # æ›´æ–°ç»Ÿè®¡
        rewards.append(episode_reward)
        steps_list.append(steps)
        high_triggers_list.append(high_triggers)
        safety_triggers_list.append(safety_triggers)
        
        if goal:
            success_count += 1
            result = "æˆåŠŸ"
        elif collision:
            collision_count += 1
            result = "ç¢°æ’"
        else:
            timeout_count += 1
            result = "è¶…æ—¶"
        
        # æ‰“å°å›åˆç»“æœ
        print(f"å›åˆ {ep+1}/{eval_episodes}: Reward={episode_reward:.2f}, " +
              f"æ­¥æ•°={steps}, ç»“æœ={result}, " +
              f"é«˜å±‚è§¦å‘={high_triggers}, å®‰å…¨è§¦å‘={safety_triggers}")
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    success_rate = success_count / eval_episodes
    collision_rate = collision_count / eval_episodes
    timeout_rate = timeout_count / eval_episodes
    avg_reward = np.mean(rewards) if rewards else 0
    avg_steps = np.mean(steps_list) if steps_list else 0
    avg_high_triggers = np.mean(high_triggers_list) if high_triggers_list else 0
    avg_safety_triggers = np.mean(safety_triggers_list) if safety_triggers_list else 0
    
    # æ‰“å°è¯„ä¼°ç»“æœ
    print("\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"   â€¢ æˆåŠŸç‡: {success_rate:.2f} ({success_count}/{eval_episodes})")
    print(f"   â€¢ ç¢°æ’ç‡: {collision_rate:.2f} ({collision_count}/{eval_episodes})")
    print(f"   â€¢ è¶…æ—¶ç‡: {timeout_rate:.2f} ({timeout_count}/{eval_episodes})")
    print(f"   â€¢ å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    print(f"   â€¢ å¹³å‡æ­¥æ•°: {avg_steps:.2f}")
    print(f"   â€¢ å¹³å‡é«˜å±‚è§¦å‘: {avg_high_triggers:.2f}")
    print(f"   â€¢ å¹³å‡å®‰å…¨è§¦å‘: {avg_safety_triggers:.2f}")
    
    # è®°å½•è¯„ä¼°æŒ‡æ ‡åˆ°æ¨¡å‹çš„TensorBoard
    if hasattr(model, 'writer'):
        model.writer.add_scalar("eval/success_rate", success_rate, epoch)
        model.writer.add_scalar("eval/collision_rate", collision_rate, epoch)
        model.writer.add_scalar("eval/avg_reward", avg_reward, epoch)
        model.writer.add_scalar("eval/avg_steps", avg_steps, epoch)
        model.writer.add_scalar("eval/avg_high_triggers", avg_high_triggers, epoch)
        model.writer.add_scalar("eval/avg_safety_triggers", avg_safety_triggers, epoch)
    
    print("="*50)
    return {
        'success_rate': success_rate,
        'collision_rate': collision_rate,
        'avg_reward': avg_reward,
        'avg_steps': avg_steps,
        'avg_high_triggers': avg_high_triggers,
        'avg_safety_triggers': avg_safety_triggers
    }


if __name__ == "__main__":
    main()  # ç¨‹åºå…¥å£ç‚¹ï¼Œå¯åŠ¨ä¸»è®­ç»ƒå‡½æ•°
