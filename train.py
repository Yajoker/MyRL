from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Tuple

import numpy as np
import torch

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from config import ConfigBundle, HighLevelRewardConfig, LowLevelRewardConfig, TrainingConfig
from integration import HierarchicalNavigationSystem
from rewards import compute_high_level_reward, compute_low_level_reward, compute_step_safety_cost
from robot_nav.SIM_ENV.sim import SIM
from replay_buffer import HighLevelReplayBuffer, ReplayBuffer


@dataclass
class SubgoalContext:
    """é«˜å±‚å­ç›®æ ‡ç”Ÿå‘½å‘¨æœŸå†…çš„ç»Ÿè®¡ä¸Šä¸‹æ–‡"""

    start_state: np.ndarray  # å­ç›®æ ‡å¼€å§‹æ—¶çš„çŠ¶æ€
    action: np.ndarray  # é€‰æ‹©çš„å­ç›®æ ‡å‡ ä½• [è·ç¦», è§’åº¦]
    world_target: np.ndarray  # å­ç›®æ ‡çš„å…¨å±€åæ ‡
    start_goal_distance: float  # å¼€å§‹æ—¶çš„ç›®æ ‡è·ç¦»
    last_goal_distance: float  # æœ€åçš„ç›®æ ‡è·ç¦»
    low_level_return: float = 0.0  # ç´¯ç§¯çš„ä½å±‚å¥–åŠ±
    steps: int = 0  # å­ç›®æ ‡æ‰§è¡Œçš„æ­¥æ•°
    subgoal_completed: bool = False  # å­ç›®æ ‡æ˜¯å¦å®Œæˆ
    last_state: Optional[np.ndarray] = None  # æœ€åçš„çŠ¶æ€
    min_dmin: float = float("inf")  # å­ç›®æ ‡æ‰§è¡ŒæœŸé—´è§‚æµ‹åˆ°çš„æœ€è¿‘éšœç¢è·ç¦»
    collision_occurred: bool = False  # æ‰§è¡ŒæœŸé—´æ˜¯å¦å‘ç”Ÿç¢°æ’
    subgoal_angle_at_start: Optional[float] = None  # å­ç›®æ ‡ç”Ÿæˆæ—¶çš„è§’åº¦
    short_cost_sum: float = 0.0  # çŸ­æœŸå®‰å…¨æˆæœ¬ç´¯è®¡
    near_obstacle_steps: int = 0  # è¿‘éšœç¢æ­¥æ•°


def compute_subgoal_world(robot_pose: Tuple[float, float, float], distance: float, angle: float) -> np.ndarray:
    """å°†ç›¸å¯¹å­ç›®æ ‡ (r, Î¸) è½¬æ¢ä¸ºå…¨å±€åæ ‡.

    Args:
        robot_pose: æœºå™¨äººä½å§¿ (x, y, theta)
        distance: å­ç›®æ ‡ç›¸å¯¹è·ç¦»
        angle: å­ç›®æ ‡ç›¸å¯¹è§’åº¦
        
    Returns:
        å­ç›®æ ‡çš„å…¨å±€åæ ‡ [x, y]
    """

    # è®¡ç®—å­ç›®æ ‡åœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„ä½ç½®
    world_x = robot_pose[0] + distance * np.cos(robot_pose[2] + angle)  # xåæ ‡è®¡ç®—
    world_y = robot_pose[1] + distance * np.sin(robot_pose[2] + angle)  # yåæ ‡è®¡ç®—
    return np.array([world_x, world_y], dtype=np.float32)  # è¿”å›ä¸–ç•Œåæ ‡æ•°ç»„


def finalize_subgoal_transition(
    context: Optional[SubgoalContext],
    buffer,
    high_cfg: HighLevelRewardConfig,
    done: bool,
    reached_goal: bool,
    collision: bool,
    timed_out: bool,
) -> Optional[Tuple[dict, None]]:
    """ç»“æŸå½“å‰å­ç›®æ ‡å¹¶ç”Ÿæˆé«˜å±‚è®­ç»ƒæ ·æœ¬.

    Args:
        context: å­ç›®æ ‡ä¸Šä¸‹æ–‡
        buffer: é«˜å±‚ç»éªŒå›æ”¾ç¼“å†²åŒº
        high_cfg: é«˜å±‚å¥–åŠ±é…ç½®
        done: æ˜¯å¦ç»ˆæ­¢
        reached_goal: æ˜¯å¦åˆ°è¾¾ç›®æ ‡
        collision: æ˜¯å¦ç¢°æ’
        timed_out: æ˜¯å¦è¶…æ—¶
        
    Returns:
        åŒ…å«å¥–åŠ±åˆ†é‡å­—å…¸çš„å…ƒç»„ï¼Œæˆ–None
    """

    # æ£€æŸ¥ä¸Šä¸‹æ–‡æœ‰æ•ˆæ€§
    if context is None or context.steps == 0:  # ä¸Šä¸‹æ–‡ä¸ºç©ºæˆ–æ­¥æ•°ä¸º0
        return None  # è¿”å›None

    # ç¡®å®šæœ€åçŠ¶æ€
    last_state = context.last_state if context.last_state is not None else context.start_state  # ä½¿ç”¨æœ€åçŠ¶æ€æˆ–å¼€å§‹çŠ¶æ€

    collision_flag = collision or context.collision_occurred

    # è®¡ç®—é«˜å±‚å¥–åŠ±
    (reward_eff, safety_cost), components = compute_high_level_reward(  # è°ƒç”¨é«˜å±‚å¥–åŠ±è®¡ç®—å‡½æ•°
        start_goal_distance=context.start_goal_distance,  # å¼€å§‹ç›®æ ‡è·ç¦»
        end_goal_distance=context.last_goal_distance,  # ç»“æŸç›®æ ‡è·ç¦»
        subgoal_step_count=context.steps,  # å­ç›®æ ‡æ­¥æ•°
        collision=collision_flag,  # æ˜¯å¦ç¢°æ’
        config=high_cfg,  # é«˜å±‚å¥–åŠ±é…ç½®
        short_cost_sum=context.short_cost_sum,
        near_obstacle_steps=context.near_obstacle_steps,
    )

    components.update(
        {
            "start_global_distance": float(context.start_goal_distance),
            "end_global_distance": float(context.last_goal_distance),
            "subgoal_steps": float(context.steps),
            "collision_flag": float(collision_flag),
            "subgoal_completed": float(context.subgoal_completed),
            "timed_out": float(timed_out),
            "reached_goal": float(reached_goal),
        }
    )

    # å°†ç»éªŒæ·»åŠ åˆ°ç¼“å†²åŒº
    buffer.add(
        context.start_state.astype(np.float32, copy=False),
        context.action.astype(np.float32, copy=False),
        float(reward_eff),
        float(safety_cost),
        float(done),
        last_state.astype(np.float32, copy=False),
    )

    return components, None


def maybe_train_high_level(
    planner,
    buffer: HighLevelReplayBuffer,
    batch_size: int,
) -> Optional[dict]:
    """å½“ç¼“å­˜æ ·æœ¬è¶³å¤Ÿæ—¶è§¦å‘ä¸€æ¬¡é«˜å±‚æ›´æ–°.

    Args:
        planner: é«˜å±‚è§„åˆ’å™¨
        buffer: é«˜å±‚ç»éªŒç¼“å†²åŒº
        batch_size: æ‰¹æ¬¡å¤§å°
        
    Returns:
        è®­ç»ƒæŒ‡æ ‡å­—å…¸æˆ–None
    """

    if buffer.size() < batch_size:
        return None

    states, actions, rewards_eff, safety_costs, dones, next_states = buffer.sample(batch_size)

    # æ›´æ–°è§„åˆ’å™¨
    metrics = planner.update_planner(
        states,
        actions,
        rewards_eff,
        safety_costs,
        dones,
        next_states,
        batch_size=batch_size,
    )  # æ›´æ–°é«˜å±‚è§„åˆ’å™¨
    return metrics  # è¿”å›è®­ç»ƒæŒ‡æ ‡


class TD3ReplayAdapter:
    """åŒ¹é…æ§åˆ¶å™¨æœŸæœ›çš„å›æ”¾ç¼“å†²åŒºAPIçš„è–„åŒ…è£…å™¨"""

    def __init__(self, buffer_size: int, random_seed: int = 666) -> None:
        """åˆå§‹åŒ–å›æ”¾ç¼“å†²åŒºé€‚é…å™¨"""
        self._buffer = ReplayBuffer(buffer_size=buffer_size, random_seed=random_seed)  # åˆ›å»ºå›æ”¾ç¼“å†²åŒº

    def add(self, state, action, reward, done, next_state) -> None:
        """å‘ç¼“å†²åŒºæ·»åŠ ç»éªŒ"""
        state_arr = np.asarray(state, dtype=np.float32)  # çŠ¶æ€æ•°ç»„
        action_arr = np.asarray(action, dtype=np.float32)  # åŠ¨ä½œæ•°ç»„
        next_state_arr = np.asarray(next_state, dtype=np.float32)  # ä¸‹ä¸€çŠ¶æ€æ•°ç»„
        reward_val = float(reward)  # å¥–åŠ±å€¼
        done_val = float(done)  # ç»ˆæ­¢æ ‡å¿—
        self._buffer.add(state_arr, action_arr, reward_val, done_val, next_state_arr)  # æ·»åŠ åˆ°ç¼“å†²åŒº

    def size(self) -> int:
        """è¿”å›ç¼“å†²åŒºå½“å‰å¤§å°"""
        return self._buffer.size()  # è¿”å›ç¼“å†²åŒºå¤§å°

    def sample(self, batch_size: int):
        """ä»ç¼“å†²åŒºé‡‡æ ·æ‰¹æ¬¡æ•°æ®"""
        states, actions, rewards, dones, next_states = self._buffer.sample_batch(batch_size)  # é‡‡æ ·æ‰¹æ¬¡æ•°æ®
        return states, actions, rewards, dones, next_states  # è¿”å›é‡‡æ ·æ•°æ®

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å†²åŒº"""
        self._buffer.clear()  # æ¸…ç©ºç¼“å†²åŒº


def get_robot_pose(sim: SIM) -> Tuple[float, float, float]:
    """ä»IR-SimåŒ…è£…å™¨ä¸­æå–æœºå™¨äººä½å§¿å¹¶è¿”å›(x, y, theta)

    Args:
        sim: ä»¿çœŸç¯å¢ƒå®ä¾‹
        
    Returns:
        æœºå™¨äººä½å§¿ (x, y, theta)
    """

    robot_state = sim.env.get_robot_state()  # è·å–æœºå™¨äººçŠ¶æ€
    return (
        float(robot_state[0].item()),  # xåæ ‡
        float(robot_state[1].item()),  # yåæ ‡
        float(robot_state[2].item()),  # èˆªå‘è§’theta
    )


def get_goal_pose(sim: SIM) -> Tuple[float, float, float]:
    """è¿”å›ä»¿çœŸç¯å¢ƒä¸­å½“å‰ç›®æ ‡ä½å§¿ (x, y, theta)."""

    goal = sim.env.robot.goal  # è·å–ç›®æ ‡çŠ¶æ€
    return (
        float(goal[0].item()),  # ç›®æ ‡xåæ ‡
        float(goal[1].item()),  # ç›®æ ‡yåæ ‡
        float(goal[2].item()) if len(goal) > 2 else 0.0,  # ç›®æ ‡è§’åº¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    )


def evaluate(
    system: HierarchicalNavigationSystem,
    sim: SIM,
    config: TrainingConfig,
    epoch: int,
    low_cfg: LowLevelRewardConfig,
) -> None:
    """è¿è¡Œæ— æ¢ç´¢å™ªå£°çš„è¯„ä¼° rollout å¹¶è®°å½•æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯.

    Args:
        system: åˆ†å±‚å¯¼èˆªç³»ç»Ÿ
        sim: ä»¿çœŸç¯å¢ƒ
        config: è®­ç»ƒé…ç½®
        epoch: å½“å‰è½®æ•°
        low_cfg: ä½å±‚å¥–åŠ±é…ç½®
    """

    print("\n" + "=" * 60)  # æ‰“å°åˆ†éš”çº¿
    print(f"ğŸ¯ EPOCH {epoch:03d} EVALUATION")  # æ‰“å°è¯„ä¼°æ ‡é¢˜
    print("=" * 60)

    # åˆå§‹åŒ–è¯„ä¼°ç»Ÿè®¡
    total_reward = 0.0  # æ€»å¥–åŠ±
    total_steps = 0  # æ€»æ­¥æ•°
    collision_count = 0  # ç¢°æ’æ¬¡æ•°
    goal_count = 0  # åˆ°è¾¾ç›®æ ‡æ¬¡æ•°
    timeout_count = 0  # è¶…æ—¶æ¬¡æ•°
    episode_rewards: List[float] = []  # æƒ…èŠ‚å¥–åŠ±åˆ—è¡¨
    episode_lengths: List[int] = []  # æƒ…èŠ‚é•¿åº¦åˆ—è¡¨
    episode_success_flags: List[bool] = []  # æƒ…èŠ‚æˆåŠŸæ ‡å¿—åˆ—è¡¨

    # è¿è¡Œè¯„ä¼°æƒ…èŠ‚
    for ep_idx in range(config.eval_episodes):  # éå†æ¯ä¸ªè¯„ä¼°æƒ…èŠ‚
        system.reset()  # é‡ç½®ç³»ç»ŸçŠ¶æ€
        latest_scan, distance, cos, sin, collision, goal, _, _ = sim.reset()  # é‡ç½®ä»¿çœŸç¯å¢ƒ
        prev_policy_action = np.zeros(2, dtype=np.float32)  # åˆå§‹åŒ–ç­–ç•¥åŠ¨ä½œ
        prev_env_action = [0.0, 0.0]  # åˆå§‹åŒ–ç‰©ç†åŠ¨ä½œ
        current_subgoal_world: Optional[np.ndarray] = None  # å½“å‰å­ç›®æ ‡ä¸–ç•Œåæ ‡
        robot_pose = get_robot_pose(sim)  # è·å–æœºå™¨äººä½å§¿
        eval_goal_pose = get_goal_pose(sim)  # è·å–è¯„ä¼°ç›®æ ‡ä½å§¿
        done = False  # ç»ˆæ­¢æ ‡å¿—
        steps = 0  # æ­¥æ•°è®¡æ•°å™¨
        episode_reward = 0.0  # æƒ…èŠ‚å¥–åŠ±
        current_subgoal_completed = False  # å½“å‰å­ç›®æ ‡å®Œæˆæ ‡å¿—

        # å•æ¬¡è¯„ä¼°æƒ…èŠ‚å¾ªç¯
        while not done and steps < config.max_steps:  # å½“æœªç»ˆæ­¢ä¸”æœªè¶…æ—¶æ—¶å¾ªç¯
            robot_pose = get_robot_pose(sim)  # è·å–æœºå™¨äººä½å§¿
            active_waypoints: list = []
            window_metrics: dict = {}
            goal_info = [distance, cos, sin]  # ç›®æ ‡ä¿¡æ¯

            scan_arr = np.asarray(latest_scan, dtype=np.float32)
            risk_index, d_min, d_percentile = system.high_level_planner.compute_risk_index(scan_arr)

            trigger_flags = system.high_level_planner.check_triggers(
                latest_scan,  # æœ€æ–°æ¿€å…‰æ•°æ®
                robot_pose,  # æœºå™¨äººä½å§¿
                goal_info,  # ç›®æ ‡ä¿¡æ¯
                risk_index=risk_index,
                current_step=steps,  # å½“å‰æ­¥æ•°
                window_metrics=None,  # çª—å£æŒ‡æ ‡
            )
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’
            should_replan = (
                system.high_level_planner.current_subgoal_world is None  # æ²¡æœ‰å½“å‰å­ç›®æ ‡
                or system.high_level_planner.should_replan(trigger_flags)  # æˆ–è§¦å‘å™¨æ¡ä»¶æ»¡è¶³
            )

            subgoal_distance: Optional[float] = None  # å­ç›®æ ‡è·ç¦»
            subgoal_angle: Optional[float] = None  # å­ç›®æ ‡è§’åº¦
            metadata = {}  # å…ƒæ•°æ®å­—å…¸

            if should_replan:  # å¦‚æœéœ€è¦é‡æ–°è§„åˆ’
                # ç”Ÿæˆæ–°å­ç›®æ ‡
                subgoal_distance, subgoal_angle, metadata = system.high_level_planner.generate_subgoal(
                    latest_scan,  # æ¿€å…‰æ•°æ®
                    distance,  # ç›®æ ‡è·ç¦»
                    cos,  # ç›®æ ‡ä½™å¼¦
                    sin,  # ç›®æ ‡æ­£å¼¦
                    robot_pose=robot_pose,  # æœºå™¨äººä½å§¿
                    current_step=steps,  # å½“å‰æ­¥æ•°
                    waypoints=None,  # æ´»åŠ¨èˆªç‚¹
                    window_metrics=None,  # çª—å£æŒ‡æ ‡
                )
                planner_world = system.high_level_planner.current_subgoal_world  # è§„åˆ’å™¨å­ç›®æ ‡ä¸–ç•Œåæ ‡
                current_subgoal_world = np.asarray(planner_world, dtype=np.float32) if planner_world is not None else None  # å½“å‰å­ç›®æ ‡ä¸–ç•Œåæ ‡
                # ä»…åœ¨æˆåŠŸç”Ÿæˆæ–°å­ç›®æ ‡åé‡ç½®äº‹ä»¶è§¦å‘æ—¶é—´
                system.high_level_planner.event_trigger.reset_time(steps)
                if current_subgoal_world is None:  # å¦‚æœæ²¡æœ‰å­ç›®æ ‡ä¸–ç•Œåæ ‡
                    current_subgoal_world = compute_subgoal_world(robot_pose, subgoal_distance, subgoal_angle)  # è®¡ç®—å­ç›®æ ‡ä¸–ç•Œåæ ‡
                current_subgoal_completed = False  # é‡ç½®å­ç›®æ ‡å®Œæˆæ ‡å¿—
            else:
                planner_world = system.high_level_planner.current_subgoal_world  # è§„åˆ’å™¨å­ç›®æ ‡ä¸–ç•Œåæ ‡
                if planner_world is not None:  # å¦‚æœå­˜åœ¨å­ç›®æ ‡ä¸–ç•Œåæ ‡
                    current_subgoal_world = np.asarray(planner_world, dtype=np.float32)  # æ›´æ–°å½“å‰å­ç›®æ ‡ä¸–ç•Œåæ ‡

            system.current_subgoal_world = current_subgoal_world  # è®¾ç½®ç³»ç»Ÿå½“å‰å­ç›®æ ‡ä¸–ç•Œåæ ‡

            relative_geometry = system.high_level_planner.get_relative_subgoal(robot_pose)  # è·å–ç›¸å¯¹å­ç›®æ ‡
            if relative_geometry[0] is None:  # å¦‚æœæ²¡æœ‰ç›¸å¯¹å‡ ä½•ä¿¡æ¯
                if should_replan and subgoal_distance is not None and subgoal_angle is not None:  # å¦‚æœéœ€è¦é‡æ–°è§„åˆ’ä¸”æœ‰å­ç›®æ ‡ä¿¡æ¯
                    relative_geometry = (subgoal_distance, subgoal_angle)  # ä½¿ç”¨æ–°ç”Ÿæˆçš„å­ç›®æ ‡
                elif system.current_subgoal is not None:  # å¦‚æœæœ‰å½“å‰å­ç›®æ ‡
                    relative_geometry = system.current_subgoal  # ä½¿ç”¨å½“å‰å­ç›®æ ‡
                else:
                    relative_geometry = (0.0, 0.0)  # é»˜è®¤å€¼

            subgoal_distance, subgoal_angle = float(relative_geometry[0]), float(relative_geometry[1])  # æ›´æ–°å­ç›®æ ‡è·ç¦»å’Œè§’åº¦
            system.current_subgoal = (subgoal_distance, subgoal_angle)  # è®¾ç½®ç³»ç»Ÿå½“å‰å­ç›®æ ‡

            # è®¡ç®—å­ç›®æ ‡è·ç¦»
            prev_subgoal_distance = None  # å‰ä¸€ä¸ªå­ç›®æ ‡è·ç¦»
            if current_subgoal_world is not None:  # å¦‚æœæœ‰å½“å‰å­ç›®æ ‡ä¸–ç•Œåæ ‡
                prev_pos = np.array(robot_pose[:2], dtype=np.float32)  # å‰ä¸€ä¸ªä½ç½®
                prev_subgoal_distance = float(np.linalg.norm(prev_pos - current_subgoal_world))  # è®¡ç®—å‰ä¸€ä¸ªå­ç›®æ ‡è·ç¦»

            # å¤„ç†ä½å±‚è§‚æµ‹
            state = system.low_level_controller.process_observation(  # å¤„ç†ä½å±‚è§‚æµ‹
                latest_scan,  # æ¿€å…‰æ•°æ®
                subgoal_distance,  # å­ç›®æ ‡è·ç¦»
                subgoal_angle,  # å­ç›®æ ‡è§’åº¦
                prev_policy_action,  # ä¸Šæ¬¡ç­–ç•¥åŠ¨ä½œ
            )

            # é¢„æµ‹åŠ¨ä½œï¼ˆæ— æ¢ç´¢å™ªå£°ï¼‰
            action = system.low_level_controller.predict_action(state, add_noise=False)  # é¢„æµ‹åŠ¨ä½œï¼ˆæ— å™ªå£°ï¼‰
            policy_action = np.clip(action, -1.0, 1.0)
            env_action = system.low_level_controller.scale_action_for_env(policy_action)
            lin_cmd = float(env_action[0])
            ang_cmd = float(env_action[1])
            lin_cmd, ang_cmd = system.apply_velocity_shielding(lin_cmd, ang_cmd, latest_scan)  # åº”ç”¨é€Ÿåº¦å±è”½

            # æ‰§è¡ŒåŠ¨ä½œ
            latest_scan, distance, cos, sin, collision, goal, _, _ = sim.step(  # æ‰§è¡Œä¸€æ­¥ä»¿çœŸ
                lin_velocity=lin_cmd,  # çº¿æ€§é€Ÿåº¦
                ang_velocity=ang_cmd,  # è§’é€Ÿåº¦
            )

            # ä½¿ç”¨åŠ¨ä½œåçš„æ¿€å…‰æ•°æ®è®¡ç®—å¥–åŠ±æ‰€éœ€çš„æœ€å°éšœç¢è·ç¦»
            post_scan = np.asarray(latest_scan, dtype=np.float32)
            finite_scan = post_scan[np.isfinite(post_scan)]
            if finite_scan.size > 0:
                risk_percentile = getattr(
                    system.high_level_planner.event_trigger, "risk_percentile", 10.0
                )
                min_obstacle_distance = float(
                    np.percentile(finite_scan, risk_percentile)
                )
            else:
                min_obstacle_distance = 8.0

            # æ›´æ–°å­ç›®æ ‡è·ç¦»
            next_pose = get_robot_pose(sim)  # è·å–ä¸‹ä¸€æ—¶åˆ»æœºå™¨äººä½å§¿
            post_window_metrics: dict = {}
            current_subgoal_distance = None  # å½“å‰å­ç›®æ ‡è·ç¦»
            if current_subgoal_world is not None:  # å¦‚æœæœ‰å½“å‰å­ç›®æ ‡ä¸–ç•Œåæ ‡
                next_pos = np.array(next_pose[:2], dtype=np.float32)  # ä¸‹ä¸€æ—¶åˆ»ä½ç½®
                current_subgoal_distance = float(np.linalg.norm(next_pos - current_subgoal_world))  # è®¡ç®—å½“å‰å­ç›®æ ‡è·ç¦»

            relative_after = system.high_level_planner.get_relative_subgoal(next_pose)  # è·å–ä¸‹ä¸€æ—¶åˆ»ç›¸å¯¹å­ç›®æ ‡
            subgoal_alignment_angle: Optional[float] = None  # å­ç›®æ ‡å¯¹é½è§’åº¦
            if relative_after[0] is not None:  # å¦‚æœæœ‰ç›¸å¯¹å‡ ä½•ä¿¡æ¯
                subgoal_alignment_angle = float(relative_after[1])  # å­ç›®æ ‡å¯¹é½è§’åº¦
                if current_subgoal_distance is None:  # å¦‚æœæ²¡æœ‰å½“å‰å­ç›®æ ‡è·ç¦»
                    current_subgoal_distance = float(relative_after[0])  # ä½¿ç”¨ç›¸å¯¹è·ç¦»

            action_delta: Optional[List[float]] = None  # åŠ¨ä½œå˜åŒ–é‡
            if prev_env_action is not None:  # å¦‚æœæœ‰ä¸Šæ¬¡åŠ¨ä½œ
                delta_lin = float(lin_cmd - prev_env_action[0])  # çº¿æ€§é€Ÿåº¦å˜åŒ–
                delta_ang = float(ang_cmd - prev_env_action[1])  # è§’é€Ÿåº¦å˜åŒ–
                action_delta = [delta_lin, delta_ang]  # åŠ¨ä½œå˜åŒ–é‡

            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            just_reached_subgoal = False  # åˆšåˆšåˆ°è¾¾å­ç›®æ ‡æ ‡å¿—
            if not current_subgoal_completed:  # å¦‚æœå½“å‰å­ç›®æ ‡æœªå®Œæˆ
                if (
                    current_subgoal_distance is not None  # å¦‚æœæœ‰å½“å‰å­ç›®æ ‡è·ç¦»
                    and current_subgoal_distance <= config.subgoal_radius  # ä¸”è·ç¦»å°äºå­ç›®æ ‡åŠå¾„
                ):
                    if prev_subgoal_distance is None:  # å¦‚æœå‰ä¸€ä¸ªå­ç›®æ ‡è·ç¦»ä¸ºNone
                        just_reached_subgoal = True  # æ ‡è®°ä¸ºåˆšåˆšåˆ°è¾¾
                    elif prev_subgoal_distance > config.subgoal_radius:  # å¦‚æœå‰ä¸€ä¸ªè·ç¦»å¤§äºåŠå¾„
                        just_reached_subgoal = True  # æ ‡è®°ä¸ºåˆšåˆšåˆ°è¾¾
            else:
                just_reached_subgoal = False  # å¦åˆ™æœªåˆ°è¾¾
            if just_reached_subgoal:  # å¦‚æœåˆšåˆšåˆ°è¾¾å­ç›®æ ‡
                current_subgoal_completed = True  # æ ‡è®°å­ç›®æ ‡å®Œæˆ
            timed_out = steps == config.max_steps - 1 and not (goal or collision)  # è¶…æ—¶åˆ¤æ–­

            # è®¡ç®—ä½å±‚å¥–åŠ±
            low_reward, _ = compute_low_level_reward(  # è®¡ç®—ä½å±‚å¥–åŠ±
                prev_subgoal_distance=prev_subgoal_distance,  # å‰ä¸€ä¸ªå­ç›®æ ‡è·ç¦»
                current_subgoal_distance=current_subgoal_distance,  # å½“å‰å­ç›®æ ‡è·ç¦»
                min_obstacle_distance=min_obstacle_distance,  # æœ€å°éšœç¢è·ç¦»
                reached_goal=goal,  # æ˜¯å¦åˆ°è¾¾ç›®æ ‡
                reached_subgoal=just_reached_subgoal,  # æ˜¯å¦åˆ°è¾¾å­ç›®æ ‡
                collision=collision,  # æ˜¯å¦ç¢°æ’
                timed_out=timed_out,  # æ˜¯å¦è¶…æ—¶
                config=low_cfg,  # ä½å±‚å¥–åŠ±é…ç½®
            )

            # æ›´æ–°ç»Ÿè®¡
            episode_reward += low_reward  # ç´¯åŠ æƒ…èŠ‚å¥–åŠ±
            steps += 1  # æ­¥æ•°åŠ 1
            prev_env_action = [lin_cmd, ang_cmd]
            prev_policy_action = policy_action.astype(np.float32, copy=False)

            # æ£€æŸ¥ç»ˆæ­¢
            if collision:  # å¦‚æœç¢°æ’
                collision_count += 1  # ç¢°æ’è®¡æ•°åŠ 1
                done = True  # æ ‡è®°ç»ˆæ­¢
            elif goal:  # å¦‚æœåˆ°è¾¾ç›®æ ‡
                goal_count += 1  # ç›®æ ‡è®¡æ•°åŠ 1
                done = True  # æ ‡è®°ç»ˆæ­¢
            elif steps >= config.max_steps:  # å¦‚æœè¾¾åˆ°æœ€å¤§æ­¥æ•°
                timeout_count += 1  # è¶…æ—¶è®¡æ•°åŠ 1
                done = True  # æ ‡è®°ç»ˆæ­¢

        # è®°å½•æƒ…èŠ‚ç»“æœ
        episode_rewards.append(episode_reward)  # æ·»åŠ æƒ…èŠ‚å¥–åŠ±
        episode_lengths.append(steps)  # æ·»åŠ æƒ…èŠ‚é•¿åº¦
        episode_success_flags.append(goal)  # æ·»åŠ æˆåŠŸæ ‡å¿—
        total_reward += episode_reward  # ç´¯åŠ æ€»å¥–åŠ±
        total_steps += steps  # ç´¯åŠ æ€»æ­¥æ•°

        status = "ğŸ¯" if goal else "ğŸ’¥" if collision else "â°"  # çŠ¶æ€è¡¨æƒ…
        print(
            f"   Evaluation Episode {ep_idx + 1:2d}/{config.eval_episodes}: {status} | "
            f"Steps: {steps:3d} | Reward: {episode_reward:7.1f}"  # æ‰“å°è¯„ä¼°ç»“æœ
        )

    # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
    avg_reward = total_reward / config.eval_episodes  # å¹³å‡å¥–åŠ±
    avg_steps = total_steps / config.eval_episodes  # å¹³å‡æ­¥æ•°
    success_rate = goal_count / config.eval_episodes * 100  # æˆåŠŸç‡
    collision_rate = collision_count / config.eval_episodes * 100  # ç¢°æ’ç‡
    timeout_rate = timeout_count / config.eval_episodes * 100  # è¶…æ—¶ç‡

    reward_std = np.std(episode_rewards) if config.eval_episodes > 1 else 0.0  # å¥–åŠ±æ ‡å‡†å·®
    steps_std = np.std(episode_lengths) if config.eval_episodes > 1 else 0.0  # æ­¥æ•°æ ‡å‡†å·®

    # è¾“å‡ºè¯„ä¼°ç»“æœ
    print("\nğŸ“ˆ Performance Summary:")  # æ€§èƒ½æ€»ç»“æ ‡é¢˜
    print(f"   â€¢ Success Rate:      {success_rate:6.1f}% ({goal_count:2d}/{config.eval_episodes:2d})")  # æˆåŠŸç‡
    print(f"   â€¢ Collision Rate:    {collision_rate:6.1f}% ({collision_count:2d}/{config.eval_episodes:2d})")  # ç¢°æ’ç‡
    print(f"   â€¢ Timeout Rate:      {timeout_rate:6.1f}% ({timeout_count:2d}/{config.eval_episodes:2d})")  # è¶…æ—¶ç‡
    print(f"   â€¢ Average Reward:    {avg_reward:8.2f} Â± {reward_std:.2f}")  # å¹³å‡å¥–åŠ±
    print(f"   â€¢ Average Steps:     {avg_steps:8.1f} Â± {steps_std:.1f}")  # å¹³å‡æ­¥æ•°

    if goal_count > 0:  # å¦‚æœæœ‰æˆåŠŸçš„æƒ…èŠ‚
        successful_rewards = [r for r, success in zip(episode_rewards, episode_success_flags) if success]  # æˆåŠŸæƒ…èŠ‚å¥–åŠ±
        avg_success_reward = np.mean(successful_rewards) if successful_rewards else 0.0  # å¹³å‡æˆåŠŸå¥–åŠ±
        print(f"   â€¢ Avg Success Reward: {avg_success_reward:8.2f}")  # æ‰“å°å¹³å‡æˆåŠŸå¥–åŠ±

    print("-" * 60)  # åˆ†éš”çº¿
    print(f"â° Evaluation completed: {config.eval_episodes} episodes")  # è¯„ä¼°å®Œæˆä¿¡æ¯
    print("=" * 60)  # åˆ†éš”çº¿

    # è®°å½•åˆ°TensorBoard
    writer = system.low_level_controller.writer  # è·å–TensorBoardå†™å…¥å™¨
    writer.add_scalar("eval/success_rate", success_rate, epoch)  # è®°å½•æˆåŠŸç‡
    writer.add_scalar("eval/collision_rate", collision_rate, epoch)  # è®°å½•ç¢°æ’ç‡
    writer.add_scalar("eval/timeout_rate", timeout_rate, epoch)  # è®°å½•è¶…æ—¶ç‡
    writer.add_scalar("eval/avg_reward", avg_reward, epoch)  # è®°å½•å¹³å‡å¥–åŠ±
    writer.add_scalar("eval/avg_steps", avg_steps, epoch)  # è®°å½•å¹³å‡æ­¥æ•°
    writer.add_scalar("eval/reward_std", reward_std, epoch)  # è®°å½•å¥–åŠ±æ ‡å‡†å·®
    writer.add_scalar("eval_raw/success_count", goal_count, epoch)  # è®°å½•æˆåŠŸè®¡æ•°
    writer.add_scalar("eval_raw/collision_count", collision_count, epoch)  # è®°å½•ç¢°æ’è®¡æ•°


def main(args=None):
    """ETHSRL+GPçš„ä¸»è¦è®­ç»ƒå¾ªç¯"""

    # ========== è®­ç»ƒé…ç½®ä¸è®¾å¤‡åˆå§‹åŒ– ==========
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # è®¾ç½®è®¾å¤‡
    bundle = ConfigBundle()  # é…ç½®åŒ…
    config = bundle.training  # è®­ç»ƒé…ç½®
    integration_config = bundle.integration  # é›†æˆé…ç½®

    raw_world = Path(config.world_file)  # ä¸–ç•Œæ–‡ä»¶è·¯å¾„
    base_dir = Path(__file__).resolve().parent  # åŸºç¡€ç›®å½•
    candidate_paths: List[Path] = []  # å€™é€‰è·¯å¾„åˆ—è¡¨
    if raw_world.is_absolute():  # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„
        candidate_paths.append(raw_world)  # æ·»åŠ ç»å¯¹è·¯å¾„
    else:
        candidate_paths.extend(  # æ·»åŠ ç›¸å¯¹è·¯å¾„å€™é€‰
            [
                base_dir / raw_world,  # åŸºç¡€ç›®å½•ä¸‹çš„è·¯å¾„
                base_dir / "worlds" / raw_world,  # worldsç›®å½•ä¸‹çš„è·¯å¾„
                base_dir.parent / "robot_nav" / "worlds" / raw_world,  # çˆ¶ç›®å½•ä¸‹çš„è·¯å¾„
            ]
        )

    world_path: Optional[Path] = None  # ä¸–ç•Œæ–‡ä»¶è·¯å¾„
    for candidate in candidate_paths:  # éå†å€™é€‰è·¯å¾„
        if candidate.exists():  # å¦‚æœè·¯å¾„å­˜åœ¨
            world_path = candidate.resolve()  # è®¾ç½®ä¸–ç•Œæ–‡ä»¶è·¯å¾„
            break

    if world_path is None:  # å¦‚æœæœªæ‰¾åˆ°ä¸–ç•Œæ–‡ä»¶
        search_list = ", ".join(str(p) for p in candidate_paths)  # æœç´¢åˆ—è¡¨
        raise FileNotFoundError(  # æŠ›å‡ºæ–‡ä»¶æœªæ‰¾åˆ°å¼‚å¸¸
            f"Unable to locate world file '{config.world_file}'. Checked: {search_list}"
        )

    world_path_str = str(world_path)  # ä¸–ç•Œæ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²

    # ========== è®­ç»ƒåˆå§‹åŒ–æ—¥å¿— ==========
    print("\n" + "="*60)  # åˆ†éš”çº¿
    print("ğŸš€ Starting ETHSRL+GP Hierarchical Navigation Training")  # è®­ç»ƒå¼€å§‹æ ‡é¢˜
    print("="*60)
    print(f"ğŸ“‹ Training Configuration:")  # è®­ç»ƒé…ç½®æ ‡é¢˜
    print(f"   â€¢ Device: {device}")  # è®¾å¤‡ä¿¡æ¯
    print(
        f"   â€¢ Max epochs: {config.max_epochs}, Episodes per epoch: {config.episodes_per_epoch}"  # æœ€å¤§è½®æ¬¡å’Œæ¯è½®æƒ…èŠ‚æ•°
    )
    print(
        f"   â€¢ Training iterations: {config.training_iterations}, Batch size: {config.batch_size}"  # è®­ç»ƒè¿­ä»£æ¬¡æ•°å’Œæ‰¹æ¬¡å¤§å°
    )
    print(f"   â€¢ Max steps per episode: {config.max_steps}")  # æ¯æƒ…èŠ‚æœ€å¤§æ­¥æ•°
    print(f"   â€¢ Train every {config.train_every_n_episodes} episodes")  # è®­ç»ƒé¢‘ç‡
    print(f"   â€¢ World file: {world_path}")  # ä¸–ç•Œæ–‡ä»¶è·¯å¾„
    print("   â€¢ Global planner: disabled (mapless mode)")  # å…¨å±€è§„åˆ’å™¨å…³é—­ï¼Œå¯ç”¨æ— å›¾å¯¼èˆª
    if config.save_every > 0:  # å¦‚æœè®¾ç½®äº†ä¿å­˜é¢‘ç‡
        print(f"   â€¢ Save models every {config.save_every} episodes")  # ä¿å­˜æ¨¡å‹é¢‘ç‡
    else:
        print("   â€¢ Save models at end of training only")  # ä»…åœ¨è®­ç»ƒç»“æŸæ—¶ä¿å­˜
    print("="*60)

    # ========== ç³»ç»Ÿåˆå§‹åŒ– ==========
    print("ğŸ”„ Initializing ETHSRL+GP system...")  # ç³»ç»Ÿåˆå§‹åŒ–ä¿¡æ¯
    system = HierarchicalNavigationSystem(  # åˆ›å»ºåˆ†å±‚å¯¼èˆªç³»ç»Ÿ
        device=device,  # è®¾å¤‡
        subgoal_threshold=config.subgoal_radius,  # å­ç›®æ ‡é˜ˆå€¼
        waypoint_lookahead=config.waypoint_lookahead,  # èˆªç‚¹å‰ç»æ•°é‡ï¼ˆå¯¹ mapless åˆ†æ”¯æ— å½±å“ï¼‰
        integration_config=integration_config,  # é›†æˆé…ç½®
    )
    replay_buffer = TD3ReplayAdapter(  # åˆ›å»ºå›æ”¾ç¼“å†²åŒºé€‚é…å™¨
        buffer_size=config.buffer_size,  # ç¼“å†²åŒºå¤§å°
        random_seed=config.random_seed or 666,  # éšæœºç§å­
    )
    print("âœ… System initialization completed")  # ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ

    # ========== ç¯å¢ƒåˆå§‹åŒ– ==========
    print("ğŸ”„ Initializing simulation environment...")  # ç¯å¢ƒåˆå§‹åŒ–ä¿¡æ¯
    sim = SIM(world_file=world_path_str, disable_plotting=False)  # åˆ›å»ºä»¿çœŸç¯å¢ƒ
    print("âœ… Environment initialization completed")  # ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ

    # ========== è®­ç»ƒç»Ÿè®¡å˜é‡åˆå§‹åŒ– ==========
    episode_reward = 0.0  # æƒ…èŠ‚å¥–åŠ±
    epoch_total_reward = 0.0  # è½®æ¬¡æ€»å¥–åŠ±
    epoch_total_steps = 0  # è½®æ¬¡æ€»æ­¥æ•°
    epoch_goal_count = 0  # è½®æ¬¡ç›®æ ‡è®¡æ•°
    epoch_collision_count = 0  # è½®æ¬¡ç¢°æ’è®¡æ•°

    # è®­ç»ƒè®¡æ•°å™¨åˆå§‹åŒ–
    episode = 0  # æƒ…èŠ‚è®¡æ•°å™¨
    epoch = 0  # è½®æ¬¡è®¡æ•°å™¨

    print("\nğŸ¬ Starting main training loop...")  # å¼€å§‹ä¸»è®­ç»ƒå¾ªç¯
    print("-" * 50)  # åˆ†éš”çº¿

    # å¥–åŠ±é…ç½®åˆå§‹åŒ–
    low_reward_cfg = bundle.low_level_reward  # ä½å±‚å¥–åŠ±é…ç½®
    high_reward_cfg = bundle.high_level_reward  # é«˜å±‚å¥–åŠ±é…ç½®
    trigger_cfg = integration_config.trigger
    high_level_buffer = HighLevelReplayBuffer(buffer_size=config.buffer_size, random_seed=config.random_seed or 666)
    current_subgoal_context: Optional[SubgoalContext] = None  # å½“å‰å­ç›®æ ‡ä¸Šä¸‹æ–‡

    # ========== ä¸»è®­ç»ƒå¾ªç¯ ==========
    while epoch < config.max_epochs:  # å½“è½®æ¬¡å°äºæœ€å¤§è½®æ¬¡æ—¶å¾ªç¯
        # é‡ç½®ç¯å¢ƒå’Œç³»ç»ŸçŠ¶æ€
        system.reset()  # é‡ç½®ç³»ç»Ÿ
        current_subgoal_context = None  # é‡ç½®å­ç›®æ ‡ä¸Šä¸‹æ–‡
        system.current_subgoal = None  # é‡ç½®å½“å‰å­ç›®æ ‡

        latest_scan, distance, cos, sin, collision, goal, _, _ = sim.reset()  # é‡ç½®ä»¿çœŸç¯å¢ƒ
        prev_policy_action = np.zeros(2, dtype=np.float32)  # å½’ä¸€åŒ–åŠ¨ä½œå†å²
        prev_env_action = [0.0, 0.0]  # ç‰©ç†åŠ¨ä½œå†å²
        current_subgoal_world: Optional[np.ndarray] = None  # å½“å‰å­ç›®æ ‡ä¸–ç•Œåæ ‡

        robot_pose = get_robot_pose(sim)  # è·å–æœºå™¨äººä½å§¿
        episode_goal_pose = get_goal_pose(sim)  # è·å–æƒ…èŠ‚ç›®æ ‡ä½å§¿

        steps = 0  # æ­¥æ•°è®¡æ•°å™¨
        episode_reward = 0.0  # æƒ…èŠ‚å¥–åŠ±
        done = False  # ç»ˆæ­¢æ ‡å¿—
        current_subgoal_completed = False  # å½“å‰å­ç›®æ ‡å®Œæˆæ ‡å¿—

        # ========== å•æ¬¡æƒ…èŠ‚å¾ªç¯ ==========
        while not done and steps < config.max_steps:  # å½“æœªç»ˆæ­¢ä¸”æœªè¶…æ—¶æ—¶å¾ªç¯
            robot_pose = get_robot_pose(sim)  # è·å–æœºå™¨äººä½å§¿
            window_metrics: dict = {}
            waypoint_sequence: list = []
            goal_info = [distance, cos, sin]  # ç›®æ ‡ä¿¡æ¯

            scan_arr = np.asarray(latest_scan, dtype=np.float32)
            risk_index, d_min, d_percentile = system.high_level_planner.compute_risk_index(scan_arr)

            trigger_flags = system.high_level_planner.check_triggers(
                latest_scan,  # æœ€æ–°æ¿€å…‰æ•°æ®
                robot_pose,  # æœºå™¨äººä½å§¿
                goal_info,  # ç›®æ ‡ä¿¡æ¯
                risk_index=risk_index,
                current_step=steps,  # å½“å‰æ­¥æ•°
                window_metrics=None,  # çª—å£æŒ‡æ ‡
            )
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’å­ç›®æ ‡
            should_replan = (
                system.high_level_planner.current_subgoal_world is None  # æ²¡æœ‰å½“å‰å­ç›®æ ‡
                or system.high_level_planner.should_replan(trigger_flags)  # æˆ–è§¦å‘å™¨æ¡ä»¶æ»¡è¶³
            )

            metadata = {}  # å…ƒæ•°æ®å­—å…¸
            subgoal_distance = None  # å­ç›®æ ‡è·ç¦»
            subgoal_angle = None  # å­ç›®æ ‡è§’åº¦

            if should_replan:  # å¦‚æœéœ€è¦é‡æ–°è§„åˆ’
                # å®Œæˆå½“å‰å­ç›®æ ‡å¹¶è®­ç»ƒ
                finalize_result = finalize_subgoal_transition(  # å®Œæˆå­ç›®æ ‡è½¬æ¢
                    current_subgoal_context,  # å½“å‰å­ç›®æ ‡ä¸Šä¸‹æ–‡
                    high_level_buffer,  # é«˜å±‚ç¼“å†²åŒº
                    high_reward_cfg,  # é«˜å±‚å¥–åŠ±é…ç½®
                    done=False,  # æœªç»ˆæ­¢
                    reached_goal=False,  # æœªåˆ°è¾¾ç›®æ ‡
                    collision=False,  # æœªç¢°æ’
                    timed_out=False,  # æœªè¶…æ—¶
                )
                if finalize_result is not None:  # å¦‚æœæœ‰ç»“æœ
                    finalize_components, _ = finalize_result  # è§£åŒ…ç»“æœ
                    metrics = maybe_train_high_level(  # å¯èƒ½è®­ç»ƒé«˜å±‚
                        system.high_level_planner,  # é«˜å±‚è§„åˆ’å™¨
                        high_level_buffer,  # é«˜å±‚ç¼“å†²åŒº
                        config.batch_size,  # æ‰¹æ¬¡å¤§å°
                    )
                    if metrics:  # å¦‚æœæœ‰è®­ç»ƒæŒ‡æ ‡
                        # è®°å½•è®­ç»ƒæŒ‡æ ‡
                        for key, value in metrics.items():  # éå†æŒ‡æ ‡
                            system.high_level_planner.writer.add_scalar(  # è®°å½•æ ‡é‡
                                f"planner/{key}",  # æŒ‡æ ‡åç§°
                                value,  # æŒ‡æ ‡å€¼
                                system.high_level_planner.iter_count,  # è¿­ä»£è®¡æ•°
                            )

                # ç”Ÿæˆæ–°å­ç›®æ ‡
                subgoal_distance, subgoal_angle, metadata = system.high_level_planner.generate_subgoal(  # ç”Ÿæˆå­ç›®æ ‡
                    latest_scan,  # æ¿€å…‰æ•°æ®
                    distance,  # ç›®æ ‡è·ç¦»
                    cos,  # ç›®æ ‡ä½™å¼¦
                    sin,  # ç›®æ ‡æ­£å¼¦
                    robot_pose=robot_pose,  # æœºå™¨äººä½å§¿
                    current_step=steps,  # å½“å‰æ­¥æ•°
                    waypoints=None,  # æ´»åŠ¨èˆªç‚¹
                    window_metrics=None,  # çª—å£æŒ‡æ ‡
                )
                planner_world = system.high_level_planner.current_subgoal_world  # è§„åˆ’å™¨å­ç›®æ ‡ä¸–ç•Œåæ ‡
                current_subgoal_world = np.asarray(planner_world, dtype=np.float32) if planner_world is not None else None  # å½“å‰å­ç›®æ ‡ä¸–ç•Œåæ ‡
                # ç”Ÿæˆæ–°çš„å­ç›®æ ‡åç»Ÿä¸€é‡ç½®äº‹ä»¶è§¦å‘æ—¶é—´
                system.high_level_planner.event_trigger.reset_time(steps)
                if current_subgoal_world is None:  # å¦‚æœæ²¡æœ‰å­ç›®æ ‡ä¸–ç•Œåæ ‡
                    current_subgoal_world = compute_subgoal_world(robot_pose, subgoal_distance, subgoal_angle)  # è®¡ç®—å­ç›®æ ‡ä¸–ç•Œåæ ‡

                # æ„å»ºé«˜å±‚çŠ¶æ€å‘é‡
                start_state = system.high_level_planner.build_state_vector(  # æ„å»ºçŠ¶æ€å‘é‡
                    latest_scan,  # æ¿€å…‰æ•°æ®
                    distance,  # ç›®æ ‡è·ç¦»
                    cos,  # ç›®æ ‡ä½™å¼¦
                    sin,  # ç›®æ ‡æ­£å¼¦
                    waypoints=waypoint_sequence,  # èˆªç‚¹åºåˆ—
                    robot_pose=robot_pose,  # æœºå™¨äººä½å§¿
                )

                # åˆ›å»ºæ–°çš„å­ç›®æ ‡ä¸Šä¸‹æ–‡
                current_subgoal_context = SubgoalContext(  # åˆ›å»ºå­ç›®æ ‡ä¸Šä¸‹æ–‡
                    start_state=start_state.astype(np.float32, copy=False),  # å¼€å§‹çŠ¶æ€
                    action=np.array([subgoal_distance, subgoal_angle], dtype=np.float32),  # å­ç›®æ ‡å‡ ä½•
                    world_target=current_subgoal_world,  # ä¸–ç•Œç›®æ ‡
                    start_goal_distance=distance,  # å¼€å§‹ç›®æ ‡è·ç¦»
                    last_goal_distance=distance,  # æœ€åç›®æ ‡è·ç¦»
                    low_level_return=0.0,  # ä½å±‚å›æŠ¥
                    steps=0,  # æ­¥æ•°
                    subgoal_completed=False,  # å­ç›®æ ‡å®Œæˆæ ‡å¿—
                    last_state=start_state.astype(np.float32, copy=False),  # æœ€åçŠ¶æ€
                    subgoal_angle_at_start=float(subgoal_angle) if subgoal_angle is not None else None,  # å­ç›®æ ‡å¼€å§‹è§’åº¦
                )
                scan_arr = np.asarray(latest_scan, dtype=np.float32)  # æ¿€å…‰æ•°æ®æ•°ç»„
                finite_scan = scan_arr[np.isfinite(scan_arr)]  # æœ‰é™å€¼æ‰«æ
                if finite_scan.size:  # å¦‚æœæœ‰æœ‰é™å€¼
                    current_subgoal_context.min_dmin = float(min(current_subgoal_context.min_dmin, finite_scan.min()))  # æ›´æ–°æœ€å°éšœç¢è·ç¦»
                current_subgoal_completed = False  # é‡ç½®å­ç›®æ ‡å®Œæˆæ ‡å¿—
            else:
                planner_world = system.high_level_planner.current_subgoal_world  # è§„åˆ’å™¨å­ç›®æ ‡ä¸–ç•Œåæ ‡
                if planner_world is not None:  # å¦‚æœå­˜åœ¨å­ç›®æ ‡ä¸–ç•Œåæ ‡
                    current_subgoal_world = np.asarray(planner_world, dtype=np.float32)  # æ›´æ–°å½“å‰å­ç›®æ ‡ä¸–ç•Œåæ ‡

            system.current_subgoal_world = current_subgoal_world  # è®¾ç½®ç³»ç»Ÿå½“å‰å­ç›®æ ‡ä¸–ç•Œåæ ‡

            relative_geometry = system.high_level_planner.get_relative_subgoal(robot_pose)  # è·å–ç›¸å¯¹å­ç›®æ ‡
            if relative_geometry[0] is None:  # å¦‚æœæ²¡æœ‰ç›¸å¯¹å‡ ä½•ä¿¡æ¯
                if should_replan and subgoal_distance is not None and subgoal_angle is not None:  # å¦‚æœéœ€è¦é‡æ–°è§„åˆ’ä¸”æœ‰å­ç›®æ ‡ä¿¡æ¯
                    relative_geometry = (subgoal_distance, subgoal_angle)  # ä½¿ç”¨æ–°ç”Ÿæˆçš„å­ç›®æ ‡
                elif system.current_subgoal is not None:  # å¦‚æœæœ‰å½“å‰å­ç›®æ ‡
                    relative_geometry = system.current_subgoal  # ä½¿ç”¨å½“å‰å­ç›®æ ‡
                else:
                    relative_geometry = (0.0, 0.0)  # é»˜è®¤å€¼

            subgoal_distance, subgoal_angle = float(relative_geometry[0]), float(relative_geometry[1])  # æ›´æ–°å­ç›®æ ‡è·ç¦»å’Œè§’åº¦
            system.current_subgoal = (subgoal_distance, subgoal_angle)  # è®¾ç½®ç³»ç»Ÿå½“å‰å­ç›®æ ‡

            # è®¡ç®—å­ç›®æ ‡è·ç¦»
            prev_subgoal_distance = None  # å‰ä¸€ä¸ªå­ç›®æ ‡è·ç¦»
            if current_subgoal_world is not None:  # å¦‚æœæœ‰å½“å‰å­ç›®æ ‡ä¸–ç•Œåæ ‡
                prev_pos = np.array(robot_pose[:2], dtype=np.float32)  # å‰ä¸€ä¸ªä½ç½®
                prev_subgoal_distance = float(np.linalg.norm(prev_pos - current_subgoal_world))  # è®¡ç®—å‰ä¸€ä¸ªå­ç›®æ ‡è·ç¦»

            # å¤„ç†ä½å±‚è§‚æµ‹
            state = system.low_level_controller.process_observation(  # å¤„ç†ä½å±‚è§‚æµ‹
                latest_scan,  # æ¿€å…‰æ•°æ®
                subgoal_distance,  # å­ç›®æ ‡è·ç¦»
                subgoal_angle,  # å­ç›®æ ‡è§’åº¦
                prev_policy_action,  # ä¸Šæ¬¡å½’ä¸€åŒ–ç­–ç•¥åŠ¨ä½œ
            )

            # é¢„æµ‹åŠ¨ä½œï¼ˆå¸¦æ¢ç´¢å™ªå£°ï¼‰
            raw_action = system.low_level_controller.predict_action(  # é¢„æµ‹åŠ¨ä½œ
                state,
                add_noise=True,  # æ·»åŠ å™ªå£°
                noise_scale=config.exploration_noise,  # å™ªå£°å°ºåº¦
            )
            policy_action = np.clip(raw_action, -1.0, 1.0)  # å½’ä¸€åŒ–ç­–ç•¥åŠ¨ä½œ

            # è½¬æ¢ä¸ºå®é™…æ§åˆ¶å‘½ä»¤ï¼ˆæœªå±è”½çš„ç¯å¢ƒåŠ¨ä½œï¼‰
            env_action = system.low_level_controller.scale_action_for_env(policy_action)
            env_lin_cmd = float(env_action[0])  # çº¿æ€§é€Ÿåº¦å‘½ä»¤
            env_ang_cmd = float(env_action[1])  # è§’é€Ÿåº¦å‘½ä»¤
            lin_cmd, ang_cmd = system.apply_velocity_shielding(env_lin_cmd, env_ang_cmd, latest_scan)  # åº”ç”¨é€Ÿåº¦å±è”½

            # æ‰§è¡ŒåŠ¨ä½œ
            latest_scan, distance, cos, sin, collision, goal, executed_action, _ = sim.step(  # æ‰§è¡Œä¸€æ­¥ä»¿çœŸ
                lin_velocity=lin_cmd,  # çº¿æ€§é€Ÿåº¦
                ang_velocity=ang_cmd,  # è§’é€Ÿåº¦
            )

            # ä½¿ç”¨åŠ¨ä½œåçš„æ¿€å…‰æ•°æ®åˆ·æ–°å¥–åŠ±æ‰€éœ€çš„æœ€å°éšœç¢è·ç¦»
            post_scan = np.asarray(latest_scan, dtype=np.float32)
            finite_scan = post_scan[np.isfinite(post_scan)]
            if finite_scan.size > 0:
                risk_percentile = getattr(
                    system.high_level_planner.event_trigger, "risk_percentile", 10.0
                )
                min_obstacle_distance = float(
                    np.percentile(finite_scan, risk_percentile)
                )
            else:
                min_obstacle_distance = 8.0

            # æ›´æ–°å­ç›®æ ‡è·ç¦»
            next_pose = get_robot_pose(sim)  # è·å–ä¸‹ä¸€æ—¶åˆ»æœºå™¨äººä½å§¿
            post_window_metrics: dict = {}
            current_subgoal_distance = None  # å½“å‰å­ç›®æ ‡è·ç¦»
            if current_subgoal_world is not None:  # å¦‚æœæœ‰å½“å‰å­ç›®æ ‡ä¸–ç•Œåæ ‡
                next_pos = np.array(next_pose[:2], dtype=np.float32)  # ä¸‹ä¸€æ—¶åˆ»ä½ç½®
                current_subgoal_distance = float(np.linalg.norm(next_pos - current_subgoal_world))  # è®¡ç®—å½“å‰å­ç›®æ ‡è·ç¦»

            relative_after = system.high_level_planner.get_relative_subgoal(next_pose)  # è·å–ä¸‹ä¸€æ—¶åˆ»ç›¸å¯¹å­ç›®æ ‡
            subgoal_alignment_angle: Optional[float] = None  # å­ç›®æ ‡å¯¹é½è§’åº¦
            if relative_after[0] is not None:  # å¦‚æœæœ‰ç›¸å¯¹å‡ ä½•ä¿¡æ¯
                subgoal_alignment_angle = float(relative_after[1])  # å­ç›®æ ‡å¯¹é½è§’åº¦
                if current_subgoal_distance is None:  # å¦‚æœæ²¡æœ‰å½“å‰å­ç›®æ ‡è·ç¦»
                    current_subgoal_distance = float(relative_after[0])  # ä½¿ç”¨ç›¸å¯¹è·ç¦»

            # Refresh subgoal geometry using post-step pose so replay stores t+1 state.
            if system.current_subgoal is not None:  # å¦‚æœæœ‰ç³»ç»Ÿå½“å‰å­ç›®æ ‡
                post_subgoal_distance = float(system.current_subgoal[0])  # åå­ç›®æ ‡è·ç¦»
                post_subgoal_angle = float(system.current_subgoal[1])  # åå­ç›®æ ‡è§’åº¦
            else:
                post_subgoal_distance = float(subgoal_distance) if subgoal_distance is not None else 0.0  # åå­ç›®æ ‡è·ç¦»
                post_subgoal_angle = float(subgoal_angle) if subgoal_angle is not None else 0.0  # åå­ç›®æ ‡è§’åº¦

            if relative_after[0] is not None:  # å¦‚æœæœ‰ç›¸å¯¹å‡ ä½•ä¿¡æ¯
                post_subgoal_distance = float(relative_after[0])  # ä½¿ç”¨ç›¸å¯¹è·ç¦»
                post_subgoal_angle = float(relative_after[1])  # ä½¿ç”¨ç›¸å¯¹è§’åº¦
            else:
                if current_subgoal_distance is not None:  # å¦‚æœæœ‰å½“å‰å­ç›®æ ‡è·ç¦»
                    post_subgoal_distance = float(current_subgoal_distance)  # ä½¿ç”¨å½“å‰å­ç›®æ ‡è·ç¦»
                if subgoal_alignment_angle is not None:  # å¦‚æœæœ‰å­ç›®æ ‡å¯¹é½è§’åº¦
                    post_subgoal_angle = float(subgoal_alignment_angle)  # ä½¿ç”¨å­ç›®æ ‡å¯¹é½è§’åº¦

            system.current_subgoal = (post_subgoal_distance, post_subgoal_angle)  # è®¾ç½®ç³»ç»Ÿå½“å‰å­ç›®æ ‡

            action_delta: Optional[List[float]] = None  # åŠ¨ä½œå˜åŒ–é‡
            if executed_action is not None and prev_env_action is not None:  # å¦‚æœæœ‰æ‰§è¡ŒåŠ¨ä½œå’Œä¸Šæ¬¡åŠ¨ä½œ
                delta_lin = float(executed_action[0] - prev_env_action[0])  # çº¿æ€§é€Ÿåº¦å˜åŒ–
                delta_ang = float(executed_action[1] - prev_env_action[1])  # è§’é€Ÿåº¦å˜åŒ–
                action_delta = [delta_lin, delta_ang]  # åŠ¨ä½œå˜åŒ–é‡

            if current_subgoal_context is not None:  # å¦‚æœæœ‰å½“å‰å­ç›®æ ‡ä¸Šä¸‹æ–‡
                current_subgoal_context.min_dmin = min(  # æ›´æ–°æœ€å°éšœç¢è·ç¦»
                    current_subgoal_context.min_dmin,
                    min_obstacle_distance,
                )
                step_cost = compute_step_safety_cost(
                    risk_index,
                    collision,
                    config=high_reward_cfg,
                )
                current_subgoal_context.short_cost_sum += step_cost
                if risk_index >= trigger_cfg.risk_near_threshold:
                    current_subgoal_context.near_obstacle_steps += 1
                if collision:  # å¦‚æœç¢°æ’
                    current_subgoal_context.collision_occurred = True  # æ ‡è®°ç¢°æ’å‘ç”Ÿ

            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            just_reached_subgoal = False  # åˆšåˆšåˆ°è¾¾å­ç›®æ ‡æ ‡å¿—
            if not current_subgoal_completed:  # å¦‚æœå½“å‰å­ç›®æ ‡æœªå®Œæˆ
                if (
                    current_subgoal_distance is not None  # å¦‚æœæœ‰å½“å‰å­ç›®æ ‡è·ç¦»
                    and current_subgoal_distance <= config.subgoal_radius  # ä¸”è·ç¦»å°äºå­ç›®æ ‡åŠå¾„
                ):
                    if prev_subgoal_distance is None:  # å¦‚æœå‰ä¸€ä¸ªå­ç›®æ ‡è·ç¦»ä¸ºNone
                        just_reached_subgoal = True  # æ ‡è®°ä¸ºåˆšåˆšåˆ°è¾¾
                    elif prev_subgoal_distance > config.subgoal_radius:  # å¦‚æœå‰ä¸€ä¸ªè·ç¦»å¤§äºåŠå¾„
                        just_reached_subgoal = True  # æ ‡è®°ä¸ºåˆšåˆšåˆ°è¾¾
            else:
                just_reached_subgoal = False  # å¦åˆ™æœªåˆ°è¾¾
            if just_reached_subgoal:  # å¦‚æœåˆšåˆšåˆ°è¾¾å­ç›®æ ‡
                current_subgoal_completed = True  # æ ‡è®°å­ç›®æ ‡å®Œæˆ
            timed_out = steps == config.max_steps - 1 and not (goal or collision)  # è¶…æ—¶åˆ¤æ–­

            # è®¡ç®—ä½å±‚å¥–åŠ±
            low_reward, _ = compute_low_level_reward(  # è®¡ç®—ä½å±‚å¥–åŠ±
                prev_subgoal_distance=prev_subgoal_distance,  # å‰ä¸€ä¸ªå­ç›®æ ‡è·ç¦»
                current_subgoal_distance=current_subgoal_distance,  # å½“å‰å­ç›®æ ‡è·ç¦»
                min_obstacle_distance=min_obstacle_distance,  # æœ€å°éšœç¢è·ç¦»
                reached_goal=goal,  # æ˜¯å¦åˆ°è¾¾ç›®æ ‡
                reached_subgoal=just_reached_subgoal,  # æ˜¯å¦åˆ°è¾¾å­ç›®æ ‡
                collision=collision,  # æ˜¯å¦ç¢°æ’
                timed_out=timed_out,  # æ˜¯å¦è¶…æ—¶
                config=low_reward_cfg,  # ä½å±‚å¥–åŠ±é…ç½®
            )

            # æ›´æ–°å¥–åŠ±ç»Ÿè®¡
            episode_reward += low_reward  # ç´¯åŠ æƒ…èŠ‚å¥–åŠ±
            epoch_total_reward += low_reward  # ç´¯åŠ è½®æ¬¡æ€»å¥–åŠ±
            epoch_total_steps += 1  # ç´¯åŠ è½®æ¬¡æ€»æ­¥æ•°

            # æ›´æ–°å­ç›®æ ‡ä¸Šä¸‹æ–‡
            if current_subgoal_context is not None:  # å¦‚æœæœ‰å½“å‰å­ç›®æ ‡ä¸Šä¸‹æ–‡
                current_subgoal_context.low_level_return += low_reward  # ç´¯åŠ ä½å±‚å›æŠ¥
                current_subgoal_context.steps += 1  # ç´¯åŠ æ­¥æ•°
                current_subgoal_context.subgoal_completed |= just_reached_subgoal  # æ›´æ–°å­ç›®æ ‡å®Œæˆæ ‡å¿—
                current_subgoal_context.last_goal_distance = distance  # æ›´æ–°æœ€åç›®æ ‡è·ç¦»
                # æ„å»ºä¸‹ä¸€çŠ¶æ€å‘é‡ï¼ˆmapless æ¨¡å¼ä¸å†ä½¿ç”¨å…¨å±€èˆªç‚¹ï¼‰
                next_state_vector = system.high_level_planner.build_state_vector(  # æ„å»ºä¸‹ä¸€çŠ¶æ€å‘é‡
                    latest_scan,  # æ¿€å…‰æ•°æ®
                    distance,  # ç›®æ ‡è·ç¦»
                    cos,  # ç›®æ ‡ä½™å¼¦
                    sin,  # ç›®æ ‡æ­£å¼¦
                    waypoints=None,  # mapless: ä¸å†æä¾›æ´»åŠ¨èˆªç‚¹
                    robot_pose=next_pose,  # ä¸‹ä¸€æ—¶åˆ»æœºå™¨äººä½å§¿
                )
                current_subgoal_context.last_state = next_state_vector.astype(np.float32, copy=False)  # æ›´æ–°æœ€åçŠ¶æ€

            # å‡†å¤‡ä¸‹ä¸€çŠ¶æ€
            next_policy_action = policy_action.astype(np.float32, copy=False)
            next_state = system.low_level_controller.process_observation(  # å¤„ç†ä¸‹ä¸€çŠ¶æ€è§‚æµ‹
                latest_scan,  # æ¿€å…‰æ•°æ®
                post_subgoal_distance,  # åå­ç›®æ ‡è·ç¦»
                post_subgoal_angle,  # åå­ç›®æ ‡è§’åº¦
                next_policy_action,  # ä¸‹ä¸€æ—¶åˆ»ç­–ç•¥åŠ¨ä½œ
            )

            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            done = collision or goal or steps == config.max_steps - 1  # ç»ˆæ­¢æ¡ä»¶

            # æ·»åŠ ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒºï¼ˆå­˜å‚¨æœªå±è”½çš„ç¯å¢ƒåŠ¨ä½œï¼‰
            scaled_env_action = np.array([env_lin_cmd, env_ang_cmd], dtype=np.float32)

            #low_reward=0.2*low_reward  # å¥–åŠ±ç¼©æ”¾
            #replay_buffer.add(state, scaled_env_action, low_reward, float(done), next_state)  # æ·»åŠ åˆ°å›æ”¾ç¼“å†²åŒº
            # âœ… ç”¨ policy_action ä½œä¸º replay buffer é‡Œçš„åŠ¨ä½œ
            replay_buffer.add(state, policy_action, low_reward, float(done), next_state)  # æ·»åŠ åˆ°å›æ”¾ç¼“å†²åŒº

            # å®šæœŸè¾“å‡ºå›æ”¾ç¼“å†²åŒºå¤§å°ä¸å¥–åŠ±
            if steps % 50 == 0:  # æ¯50æ­¥è¾“å‡ºä¸€æ¬¡
                buffer_size = replay_buffer.size()  # ç¼“å†²åŒºå¤§å°
                print(
                    f"ğŸƒ Training | Epoch {epoch:2d}/{config.max_epochs} | "  # è®­ç»ƒä¿¡æ¯
                    f"Episode {episode:3d}/{config.max_epochs*config.episodes_per_epoch} | "
                    f"Step {steps:3d}/{config.max_steps} | "
                    f"Reward: {low_reward:7.2f} | Buffer: {buffer_size:6d}"
                )

            prev_policy_action = next_policy_action  # æ›´æ–°ç­–ç•¥åŠ¨ä½œ
            prev_env_action = [executed_action[0], executed_action[1]]  # æ›´æ–°ç‰©ç†åŠ¨ä½œ
            steps += 1  # æ­¥æ•°åŠ 1

        # ========== æƒ…èŠ‚ç»“æŸå¤„ç† ==========
        timed_out_episode = not goal and not collision and steps >= config.max_steps  # è¶…æ—¶æƒ…èŠ‚åˆ¤æ–­

        # å®Œæˆæœ€åä¸€ä¸ªå­ç›®æ ‡
        finalize_result = finalize_subgoal_transition(  # å®Œæˆå­ç›®æ ‡è½¬æ¢
            current_subgoal_context,  # å½“å‰å­ç›®æ ‡ä¸Šä¸‹æ–‡
            high_level_buffer,  # é«˜å±‚ç¼“å†²åŒº
            high_reward_cfg,  # é«˜å±‚å¥–åŠ±é…ç½®
            done=True,  # ç»ˆæ­¢
            reached_goal=goal,  # åˆ°è¾¾ç›®æ ‡
            collision=collision,  # ç¢°æ’
            timed_out=timed_out_episode,  # è¶…æ—¶
        )
        if finalize_result is not None:  # å¦‚æœæœ‰ç»“æœ
            finalize_components, _ = finalize_result  # è§£åŒ…ç»“æœ
            metrics = maybe_train_high_level(  # å¯èƒ½è®­ç»ƒé«˜å±‚
                system.high_level_planner,  # é«˜å±‚è§„åˆ’å™¨
                high_level_buffer,  # é«˜å±‚ç¼“å†²åŒº
                config.batch_size,  # æ‰¹æ¬¡å¤§å°
            )
            if metrics:  # å¦‚æœæœ‰è®­ç»ƒæŒ‡æ ‡
                for key, value in metrics.items():  # éå†æŒ‡æ ‡
                    system.high_level_planner.writer.add_scalar(  # è®°å½•æ ‡é‡
                        f"planner/{key}",  # æŒ‡æ ‡åç§°
                        value,  # æŒ‡æ ‡å€¼
                        system.high_level_planner.iter_count,  # è¿­ä»£è®¡æ•°
                    )
        
        # é‡ç½®å­ç›®æ ‡ä¸Šä¸‹æ–‡
        current_subgoal_context = None  # é‡ç½®å­ç›®æ ‡ä¸Šä¸‹æ–‡
        current_subgoal_world = None  # é‡ç½®å­ç›®æ ‡ä¸–ç•Œåæ ‡

        # æ›´æ–°ç»Ÿè®¡
        if goal:  # å¦‚æœåˆ°è¾¾ç›®æ ‡
            epoch_goal_count += 1  # è½®æ¬¡ç›®æ ‡è®¡æ•°åŠ 1
        if collision:  # å¦‚æœç¢°æ’
            epoch_collision_count += 1  # è½®æ¬¡ç¢°æ’è®¡æ•°åŠ 1

        # è¾“å‡ºæƒ…èŠ‚ç»“æœ
        status = "ğŸ¯ GOAL" if goal else "ğŸ’¥ COLLISION" if collision else "â° TIMEOUT"  # çŠ¶æ€ä¿¡æ¯
        print(
            f"   Episode {episode:3d} finished: {status} | "  # æƒ…èŠ‚å®Œæˆä¿¡æ¯
            f"Steps: {steps:3d} | Total Reward: {episode_reward:7.1f}"
        )

        # è®°å½•åˆ°TensorBoard
        writer = system.low_level_controller.writer  # è·å–TensorBoardå†™å…¥å™¨
        writer.add_scalar("train/episode_reward", episode_reward, episode)  # è®°å½•æƒ…èŠ‚å¥–åŠ±

        # ========== è®­ç»ƒä½å±‚æ§åˆ¶å™¨ ==========
        if (
            replay_buffer.size() >= config.min_buffer_size  # å¦‚æœç¼“å†²åŒºå¤§å°è¾¾åˆ°æœ€å°å€¼
            and episode % config.train_every_n_episodes == 0  # ä¸”è¾¾åˆ°è®­ç»ƒé¢‘ç‡
        ):
            current_buffer_size = replay_buffer.size()  # å½“å‰ç¼“å†²åŒºå¤§å°
            print(f"   ğŸ”„ Training model... (Buffer: {current_buffer_size} samples)")  # è®­ç»ƒæ¨¡å‹ä¿¡æ¯

            # æ‰§è¡Œå¤šæ¬¡è®­ç»ƒè¿­ä»£
            for _ in range(config.training_iterations):  # è®­ç»ƒè¿­ä»£æ¬¡æ•°
                system.low_level_controller.update(  # æ›´æ–°ä½å±‚æ§åˆ¶å™¨
                    replay_buffer,  # å›æ”¾ç¼“å†²åŒº
                    batch_size=config.batch_size,  # æ‰¹æ¬¡å¤§å°
                    discount=0.99,  # æŠ˜æ‰£å› å­
                    tau=0.005,    # è½¯æ›´æ–°å‚æ•°
                    policy_noise=0.2,  # ç­–ç•¥å™ªå£°
                    noise_clip=0.5,  # å™ªå£°è£å‰ª
                    policy_freq=2,   # ç­–ç•¥é¢‘ç‡
                )
            print("   âœ… Training completed")  # è®­ç»ƒå®Œæˆä¿¡æ¯

        episode += 1  # æƒ…èŠ‚è®¡æ•°å™¨åŠ 1

        # ========== æ¨¡å‹ä¿å­˜ ==========
        if config.save_every > 0 and episode % config.save_every == 0:  # å¦‚æœè¾¾åˆ°ä¿å­˜é¢‘ç‡
            print(f"   ğŸ’¾ Saving checkpoints after episode {episode}")  # ä¿å­˜æ£€æŸ¥ç‚¹ä¿¡æ¯
            system.high_level_planner.save_model(  # ä¿å­˜é«˜å±‚è§„åˆ’å™¨æ¨¡å‹
                filename=system.high_level_planner.model_name,  # æ¨¡å‹åç§°
                directory=system.high_level_planner.save_directory,  # ä¿å­˜ç›®å½•
            )
            system.low_level_controller.save_model(  # ä¿å­˜ä½å±‚æ§åˆ¶å™¨æ¨¡å‹
                filename=system.low_level_controller.model_name,  # æ¨¡å‹åç§°
                directory=system.low_level_controller.save_directory,  # ä¿å­˜ç›®å½•
            )

        # ========== è½®æ¬¡ç»“æŸå¤„ç† ==========
        if episode % config.episodes_per_epoch == 0:  # å¦‚æœè¾¾åˆ°è½®æ¬¡æƒ…èŠ‚æ•°
            # è®¡ç®—è½®æ¬¡ç»Ÿè®¡
            epoch_avg_reward = epoch_total_reward / config.episodes_per_epoch  # è½®æ¬¡å¹³å‡å¥–åŠ±
            epoch_success_rate = epoch_goal_count / config.episodes_per_epoch * 100  # è½®æ¬¡æˆåŠŸç‡
            epoch_collision_rate = epoch_collision_count / config.episodes_per_epoch * 100  # è½®æ¬¡ç¢°æ’ç‡

            # è¾“å‡ºè½®æ¬¡æ€»ç»“
            print("\n" + "=" * 60)  # åˆ†éš”çº¿
            print(f"ğŸ“Š EPOCH {epoch:03d} TRAINING SUMMARY")  # è½®æ¬¡æ€»ç»“æ ‡é¢˜
            print("=" * 60)
            print(
                f"   â€¢ Success Rate:    {epoch_success_rate:6.1f}% "  # æˆåŠŸç‡
                f"({epoch_goal_count:2d}/{config.episodes_per_epoch:2d})"
            )
            print(
                f"   â€¢ Collision Rate:  {epoch_collision_rate:6.1f}% "  # ç¢°æ’ç‡
                f"({epoch_collision_count:2d}/{config.episodes_per_epoch:2d})"
            )
            print(f"   â€¢ Average Reward:  {epoch_avg_reward:8.2f}")  # å¹³å‡å¥–åŠ±
            print(f"   â€¢ Total Steps:     {epoch_total_steps:8d}")  # æ€»æ­¥æ•°
            print(f"   â€¢ Buffer Size:     {replay_buffer.size():8d}")  # ç¼“å†²åŒºå¤§å°
            print("=" * 60)

            # é‡ç½®è½®æ¬¡ç»Ÿè®¡
            epoch_total_reward = 0.0  # é‡ç½®è½®æ¬¡æ€»å¥–åŠ±
            epoch_total_steps = 0  # é‡ç½®è½®æ¬¡æ€»æ­¥æ•°
            epoch_goal_count = 0  # é‡ç½®è½®æ¬¡ç›®æ ‡è®¡æ•°
            epoch_collision_count = 0  # é‡ç½®è½®æ¬¡ç¢°æ’è®¡æ•°

            epoch += 1  # è½®æ¬¡è®¡æ•°å™¨åŠ 1

            # æ‰§è¡Œè¯„ä¼°
            evaluate(system, sim, config, epoch, low_reward_cfg)  # æ‰§è¡Œè¯„ä¼°

    # ========== è®­ç»ƒå®Œæˆå¤„ç† ==========
    print("\nğŸ’¾ Saving final checkpoints...")  # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
    system.high_level_planner.save_model(  # ä¿å­˜é«˜å±‚è§„åˆ’å™¨æ¨¡å‹
        filename=system.high_level_planner.model_name,  # æ¨¡å‹åç§°
        directory=system.high_level_planner.save_directory,  # ä¿å­˜ç›®å½•
    )
    system.low_level_controller.save_model(  # ä¿å­˜ä½å±‚æ§åˆ¶å™¨æ¨¡å‹
        filename=system.low_level_controller.model_name,  # æ¨¡å‹åç§°
        directory=system.low_level_controller.save_directory,  # ä¿å­˜ç›®å½•
    )

    print("\n" + "="*60)  # åˆ†éš”çº¿
    print("ğŸ‰ ETHSRL+GP Training Completed!")  # è®­ç»ƒå®Œæˆä¿¡æ¯
    print("="*60)
    print(f"ğŸ“ˆ Final performance after {config.max_epochs} epochs")  # æœ€ç»ˆæ€§èƒ½ä¿¡æ¯
    print("="*60)


if __name__ == "__main__":
    main()  # è¿è¡Œä¸»å‡½æ•°
