"""
ETHSRL+GPåˆ†å±‚å¯¼èˆªç³»ç»Ÿçš„è®­ç»ƒå…¥å£ç‚¹

è¯¥è„šæœ¬éµå¾ªåŸå§‹``robot_nav/rl_train.py``çš„ç»“æ„ï¼Œ
åŒæ—¶é›†æˆäº†æ–°å®ç°çš„é«˜å±‚è§„åˆ’å™¨å’Œä½å±‚æ§åˆ¶å™¨ã€‚
"""

from dataclasses import dataclass
from typing import List, Optional, Tuple

import numpy as np
import torch

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from ethsrl.core.integration import HierarchicalNavigationSystem
from ethsrl.core.rewards import (
    HighLevelRewardConfig,
    LowLevelRewardConfig,
    compute_high_level_reward,
    compute_low_level_reward,
)
from robot_nav.SIM_ENV.sim import SIM
from robot_nav.replay_buffer import ReplayBuffer


@dataclass
class TrainingConfig:
    """è®­ç»ƒè¶…å‚æ•°é…ç½®å®¹å™¨"""

    buffer_size: int = 100_000  # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
    batch_size: int = 64  # è®­ç»ƒæ‰¹æ¬¡å¤§å°
    max_epochs: int = 60  # æœ€å¤§è®­ç»ƒè½®æ•°
    episodes_per_epoch: int = 70  # æ¯è½®è®­ç»ƒçš„æƒ…èŠ‚æ•°
    max_steps: int = 300  # æ¯ä¸ªæƒ…èŠ‚çš„æœ€å¤§æ­¥æ•°
    train_every_n_episodes: int = 2  # æ¯Nä¸ªæƒ…èŠ‚è®­ç»ƒä¸€æ¬¡
    training_iterations: int = 80  # æ¯æ¬¡è®­ç»ƒçš„è¿­ä»£æ¬¡æ•°
    exploration_noise: float = 0.2  # æ¢ç´¢å™ªå£°å¼ºåº¦
    min_buffer_size: int = 1_000  # å¼€å§‹è®­ç»ƒçš„æœ€å°ç¼“å†²åŒºå¤§å°
    max_lin_velocity: float = 0.5  # æœ€å¤§çº¿é€Ÿåº¦
    max_ang_velocity: float = 1.0  # æœ€å¤§è§’é€Ÿåº¦
    eval_episodes: int = 10  # è¯„ä¼°æ—¶ä½¿ç”¨çš„æƒ…èŠ‚æ•°
    subgoal_radius: float = 0.5  # åˆ¤å®šå­ç›®æ ‡è¾¾æˆçš„è·ç¦»é˜ˆå€¼
    save_every: int = 5  # æ¯éš”å¤šå°‘ä¸ªæƒ…èŠ‚ä¿å­˜ä¸€æ¬¡æ¨¡å‹ï¼ˆ<=0 è¡¨ç¤ºä»…æœ€ç»ˆä¿å­˜ï¼‰


@dataclass
class SubgoalContext:
    """é«˜å±‚å­ç›®æ ‡ç”Ÿå‘½å‘¨æœŸå†…çš„ç»Ÿè®¡ä¸Šä¸‹æ–‡"""

    start_state: np.ndarray  # å­ç›®æ ‡å¼€å§‹æ—¶çš„çŠ¶æ€
    action: np.ndarray  # é€‰æ‹©çš„å­ç›®æ ‡åŠ¨ä½œ [è·ç¦», è§’åº¦]
    world_target: np.ndarray  # å­ç›®æ ‡çš„å…¨å±€åæ ‡
    start_goal_distance: float  # å¼€å§‹æ—¶çš„ç›®æ ‡è·ç¦»
    last_goal_distance: float  # æœ€åçš„ç›®æ ‡è·ç¦»
    low_level_return: float = 0.0  # ç´¯ç§¯çš„ä½å±‚å¥–åŠ±
    steps: int = 0  # å­ç›®æ ‡æ‰§è¡Œçš„æ­¥æ•°
    subgoal_completed: bool = False  # å­ç›®æ ‡æ˜¯å¦å®Œæˆ
    last_state: Optional[np.ndarray] = None  # æœ€åçš„çŠ¶æ€


def compute_subgoal_world(robot_pose: Tuple[float, float, float], distance: float, angle: float) -> np.ndarray:
    """å°†ç›¸å¯¹å­ç›®æ ‡ (r, Î¸) è½¬æ¢ä¸ºå…¨å±€åæ ‡.

    Args:
        robot_pose: æœºå™¨äººä½å§¿ (x, y, theta)
        distance: å­ç›®æ ‡ç›¸å¯¹è·ç¦»
        angle: å­ç›®æ ‡ç›¸å¯¹è§’åº¦
        
    Returns:
        å­ç›®æ ‡çš„å…¨å±€åæ ‡ [x, y]
    """

    # è®¡ç®—å­ç›®æ ‡åœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„ä½ç½®
    world_x = robot_pose[0] + distance * np.cos(robot_pose[2] + angle)
    world_y = robot_pose[1] + distance * np.sin(robot_pose[2] + angle)
    return np.array([world_x, world_y], dtype=np.float32)


def finalize_subgoal_transition(
    context: Optional[SubgoalContext],
    buffer: List[Tuple[np.ndarray, np.ndarray, float, np.ndarray, float]],
    high_cfg: HighLevelRewardConfig,
    done: bool,
    reached_goal: bool,
    collision: bool,
    timed_out: bool,
) -> Optional[dict]:
    """ç»“æŸå½“å‰å­ç›®æ ‡å¹¶ç”Ÿæˆé«˜å±‚è®­ç»ƒæ ·æœ¬.

    Args:
        context: å­ç›®æ ‡ä¸Šä¸‹æ–‡
        buffer: é«˜å±‚ç»éªŒå›æ”¾ç¼“å†²åŒº
        high_cfg: é«˜å±‚å¥–åŠ±é…ç½®
        done: æ˜¯å¦ç»ˆæ­¢
        reached_goal: æ˜¯å¦åˆ°è¾¾ç›®æ ‡
        collision: æ˜¯å¦ç¢°æ’
        timed_out: æ˜¯å¦è¶…æ—¶
        
    Returns:
        å¥–åŠ±åˆ†é‡å­—å…¸æˆ–None
    """

    # æ£€æŸ¥ä¸Šä¸‹æ–‡æœ‰æ•ˆæ€§
    if context is None or context.steps == 0:
        return None

    # ç¡®å®šæœ€åçŠ¶æ€
    last_state = context.last_state if context.last_state is not None else context.start_state

    # è®¡ç®—é«˜å±‚å¥–åŠ±
    reward, components = compute_high_level_reward(
        start_goal_distance=context.start_goal_distance,
        end_goal_distance=context.last_goal_distance,
        subgoal_completed=context.subgoal_completed,
        reached_goal=reached_goal,
        collision=collision,
        timed_out=timed_out,
        config=high_cfg,
    )

    # å°†ç»éªŒæ·»åŠ åˆ°ç¼“å†²åŒº
    buffer.append(
        (
            context.start_state.astype(np.float32, copy=False),  # å¼€å§‹çŠ¶æ€
            context.action.astype(np.float32, copy=False),  # å­ç›®æ ‡åŠ¨ä½œ
            float(reward),  # å¥–åŠ±
            last_state.astype(np.float32, copy=False),  # ç»“æŸçŠ¶æ€
            float(done),  # ç»ˆæ­¢æ ‡å¿—
        )
    )

    return components


def maybe_train_high_level(
    planner,
    buffer: List[Tuple[np.ndarray, np.ndarray, float, np.ndarray, float]],
    batch_size: int,
) -> Optional[dict]:
    """å½“ç¼“å­˜æ ·æœ¬è¶³å¤Ÿæ—¶è§¦å‘ä¸€æ¬¡é«˜å±‚æ›´æ–°.

    Args:
        planner: é«˜å±‚è§„åˆ’å™¨
        buffer: é«˜å±‚ç»éªŒç¼“å†²åŒº
        batch_size: æ‰¹æ¬¡å¤§å°
        
    Returns:
        è®­ç»ƒæŒ‡æ ‡å­—å…¸æˆ–None
    """

    # æ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦è¶³å¤Ÿ
    if len(buffer) < batch_size:
        return None

    # æå–æ‰¹æ¬¡æ•°æ®
    batch = buffer[:batch_size]
    del buffer[:batch_size]  # ç§»é™¤å·²ä½¿ç”¨çš„æ ·æœ¬

    # ç»„ç»‡æ‰¹æ¬¡æ•°æ®
    states = np.stack([entry[0] for entry in batch])
    actions = np.stack([entry[1] for entry in batch])
    rewards = np.array([entry[2] for entry in batch], dtype=np.float32)
    next_states = np.stack([entry[3] for entry in batch])
    dones = np.array([entry[4] for entry in batch], dtype=np.float32)

    # æ›´æ–°è§„åˆ’å™¨
    metrics = planner.update_planner(states, actions, rewards, dones, next_states, batch_size=batch_size)
    return metrics


class TD3ReplayAdapter:
    """åŒ¹é…æ§åˆ¶å™¨æœŸæœ›çš„å›æ”¾ç¼“å†²åŒºAPIçš„è–„åŒ…è£…å™¨"""

    def __init__(self, buffer_size: int, random_seed: int = 666) -> None:
        """åˆå§‹åŒ–å›æ”¾ç¼“å†²åŒºé€‚é…å™¨"""
        self._buffer = ReplayBuffer(buffer_size=buffer_size, random_seed=random_seed)

    def add(self, state, action, reward, done, next_state) -> None:
        """å‘ç¼“å†²åŒºæ·»åŠ ç»éªŒ"""
        self._buffer.add(state, action, reward, done, next_state)

    def size(self) -> int:
        """è¿”å›ç¼“å†²åŒºå½“å‰å¤§å°"""
        return self._buffer.size()

    def sample(self, batch_size: int):
        """ä»ç¼“å†²åŒºé‡‡æ ·æ‰¹æ¬¡æ•°æ®"""
        states, actions, rewards, dones, next_states = self._buffer.sample_batch(batch_size)
        return states, actions, rewards, dones, next_states

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å†²åŒº"""
        self._buffer.clear()


def get_robot_pose(sim: SIM) -> Tuple[float, float, float]:
    """ä»IR-SimåŒ…è£…å™¨ä¸­æå–æœºå™¨äººä½å§¿å¹¶è¿”å›(x, y, theta)

    Args:
        sim: ä»¿çœŸç¯å¢ƒå®ä¾‹
        
    Returns:
        æœºå™¨äººä½å§¿ (x, y, theta)
    """

    robot_state = sim.env.get_robot_state()  # è·å–æœºå™¨äººçŠ¶æ€
    return (
        float(robot_state[0].item()),  # xåæ ‡
        float(robot_state[1].item()),  # yåæ ‡
        float(robot_state[2].item()),  # èˆªå‘è§’theta
    )


def evaluate(
    system: HierarchicalNavigationSystem,
    sim: SIM,
    config: TrainingConfig,
    epoch: int,
    low_cfg: LowLevelRewardConfig,
) -> None:
    """è¿è¡Œæ— æ¢ç´¢å™ªå£°çš„è¯„ä¼° rollout å¹¶è®°å½•æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯.

    Args:
        system: åˆ†å±‚å¯¼èˆªç³»ç»Ÿ
        sim: ä»¿çœŸç¯å¢ƒ
        config: è®­ç»ƒé…ç½®
        epoch: å½“å‰è½®æ•°
        low_cfg: ä½å±‚å¥–åŠ±é…ç½®
    """

    print("\n" + "=" * 60)
    print(f"ğŸ¯ EPOCH {epoch:03d} EVALUATION")
    print("=" * 60)

    # åˆå§‹åŒ–è¯„ä¼°ç»Ÿè®¡
    total_reward = 0.0
    total_steps = 0
    collision_count = 0
    goal_count = 0
    timeout_count = 0
    episode_rewards: List[float] = []
    episode_lengths: List[int] = []
    episode_success_flags: List[bool] = []

    # è¿è¡Œè¯„ä¼°æƒ…èŠ‚
    for ep_idx in range(config.eval_episodes):
        system.reset()  # é‡ç½®ç³»ç»ŸçŠ¶æ€
        latest_scan, distance, cos, sin, collision, goal, prev_action, _ = sim.reset()
        prev_action = [0.0, 0.0]  # åˆå§‹åŒ–åŠ¨ä½œ
        current_subgoal_world: Optional[np.ndarray] = None
        done = False
        steps = 0
        episode_reward = 0.0

        # å•æ¬¡è¯„ä¼°æƒ…èŠ‚å¾ªç¯
        while not done and steps < config.max_steps:
            robot_pose = get_robot_pose(sim)
            goal_info = [distance, cos, sin]

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’
            should_replan = (
                system.high_level_planner.current_subgoal_world is None
                or system.high_level_planner.check_triggers(
                    latest_scan,
                    robot_pose,
                    goal_info,
                    prev_action=prev_action,
                    current_step=steps,
                )
            )

            subgoal_distance: Optional[float] = None
            subgoal_angle: Optional[float] = None

            subgoal_distance: Optional[float] = None
            subgoal_angle: Optional[float] = None

            if should_replan:
                # ç”Ÿæˆæ–°å­ç›®æ ‡
                subgoal_distance, subgoal_angle = system.high_level_planner.generate_subgoal(
                    latest_scan,
                    distance,
                    cos,
                    sin,
                    prev_action=prev_action,
                    robot_pose=robot_pose,
                    current_step=steps,
                )
                planner_world = system.high_level_planner.current_subgoal_world
                current_subgoal_world = np.asarray(planner_world, dtype=np.float32) if planner_world is not None else None
                system.high_level_planner.event_trigger.reset_time(steps)
                if current_subgoal_world is None:
                    current_subgoal_world = compute_subgoal_world(robot_pose, subgoal_distance, subgoal_angle)
            else:
                planner_world = system.high_level_planner.current_subgoal_world
                if planner_world is not None:
                    current_subgoal_world = np.asarray(planner_world, dtype=np.float32)

            system.current_subgoal_world = current_subgoal_world

            relative_geometry = system.high_level_planner.get_relative_subgoal(robot_pose)
            if relative_geometry[0] is None:
                if should_replan and subgoal_distance is not None and subgoal_angle is not None:
                    relative_geometry = (subgoal_distance, subgoal_angle)
                elif system.current_subgoal is not None:
                    relative_geometry = system.current_subgoal
                else:
                    relative_geometry = (0.0, 0.0)

            subgoal_distance, subgoal_angle = float(relative_geometry[0]), float(relative_geometry[1])
            system.current_subgoal = (subgoal_distance, subgoal_angle)

            # è®¡ç®—å­ç›®æ ‡è·ç¦»
            prev_subgoal_distance = None
            if current_subgoal_world is not None:
                prev_pos = np.array(robot_pose[:2], dtype=np.float32)
                prev_subgoal_distance = float(np.linalg.norm(prev_pos - current_subgoal_world))

            # å¤„ç†ä½å±‚è§‚æµ‹
            state = system.low_level_controller.process_observation(
                latest_scan,
                subgoal_distance,
                subgoal_angle,
                prev_action,
            )

            # é¢„æµ‹åŠ¨ä½œï¼ˆæ— æ¢ç´¢å™ªå£°ï¼‰
            action = system.low_level_controller.predict_action(state, add_noise=False)
            lin_cmd = float(np.clip((action[0] + 1.0) / 4.0, 0.0, config.max_lin_velocity))
            ang_cmd = float(np.clip(action[1], -config.max_ang_velocity, config.max_ang_velocity))

            # æ‰§è¡ŒåŠ¨ä½œ
            latest_scan, distance, cos, sin, collision, goal, _, _ = sim.step(
                lin_velocity=lin_cmd,
                ang_velocity=ang_cmd,
            )

            # æ›´æ–°å­ç›®æ ‡è·ç¦»
            next_pose = get_robot_pose(sim)
            current_subgoal_distance = None
            if current_subgoal_world is not None:
                next_pos = np.array(next_pose[:2], dtype=np.float32)
                current_subgoal_distance = float(np.linalg.norm(next_pos - current_subgoal_world))

            # è®¡ç®—æœ€å°éšœç¢ç‰©è·ç¦»
            scan_arr = np.asarray(latest_scan, dtype=np.float32)
            finite_scan = scan_arr[np.isfinite(scan_arr)]
            min_obstacle_distance = float(finite_scan.min()) if finite_scan.size else 8.0

            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            just_reached_subgoal = False
            if (
                current_subgoal_distance is not None
                and current_subgoal_distance <= config.subgoal_radius
            ):
                if prev_subgoal_distance is None:
                    just_reached_subgoal = True
                elif prev_subgoal_distance > config.subgoal_radius:
                    just_reached_subgoal = True
            timed_out = steps == config.max_steps - 1 and not (goal or collision)

            # è®¡ç®—ä½å±‚å¥–åŠ±
            low_reward, _ = compute_low_level_reward(
                prev_subgoal_distance=prev_subgoal_distance,
                current_subgoal_distance=current_subgoal_distance,
                min_obstacle_distance=min_obstacle_distance,
                reached_goal=goal,
                reached_subgoal=just_reached_subgoal,
                collision=collision,
                timed_out=timed_out,
                config=low_cfg,
            )

            # æ›´æ–°ç»Ÿè®¡
            episode_reward += low_reward
            steps += 1
            prev_action = [lin_cmd, ang_cmd]

            # æ£€æŸ¥ç»ˆæ­¢
            if collision:
                collision_count += 1
                done = True
            elif goal:
                goal_count += 1
                done = True
            elif steps >= config.max_steps:
                timeout_count += 1
                done = True

        # è®°å½•æƒ…èŠ‚ç»“æœ
        episode_rewards.append(episode_reward)
        episode_lengths.append(steps)
        episode_success_flags.append(goal)
        total_reward += episode_reward
        total_steps += steps

        status = "ğŸ¯" if goal else "ğŸ’¥" if collision else "â°"
        print(
            f"   Evaluation Episode {ep_idx + 1:2d}/{config.eval_episodes}: {status} | "
            f"Steps: {steps:3d} | Reward: {episode_reward:7.1f}"
        )

    # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
    avg_reward = total_reward / config.eval_episodes
    avg_steps = total_steps / config.eval_episodes
    success_rate = goal_count / config.eval_episodes * 100
    collision_rate = collision_count / config.eval_episodes * 100
    timeout_rate = timeout_count / config.eval_episodes * 100

    reward_std = np.std(episode_rewards) if config.eval_episodes > 1 else 0.0
    steps_std = np.std(episode_lengths) if config.eval_episodes > 1 else 0.0

    # è¾“å‡ºè¯„ä¼°ç»“æœ
    print("\nğŸ“ˆ Performance Summary:")
    print(f"   â€¢ Success Rate:      {success_rate:6.1f}% ({goal_count:2d}/{config.eval_episodes:2d})")
    print(f"   â€¢ Collision Rate:    {collision_rate:6.1f}% ({collision_count:2d}/{config.eval_episodes:2d})")
    print(f"   â€¢ Timeout Rate:      {timeout_rate:6.1f}% ({timeout_count:2d}/{config.eval_episodes:2d})")
    print(f"   â€¢ Average Reward:    {avg_reward:8.2f} Â± {reward_std:.2f}")
    print(f"   â€¢ Average Steps:     {avg_steps:8.1f} Â± {steps_std:.1f}")

    if goal_count > 0:
        successful_rewards = [r for r, success in zip(episode_rewards, episode_success_flags) if success]
        avg_success_reward = np.mean(successful_rewards) if successful_rewards else 0.0
        print(f"   â€¢ Avg Success Reward: {avg_success_reward:8.2f}")

    print("-" * 60)
    print(f"â° Evaluation completed: {config.eval_episodes} episodes")
    print("=" * 60)

    # è®°å½•åˆ°TensorBoard
    writer = system.low_level_controller.writer
    writer.add_scalar("eval/success_rate", success_rate, epoch)
    writer.add_scalar("eval/collision_rate", collision_rate, epoch)
    writer.add_scalar("eval/timeout_rate", timeout_rate, epoch)
    writer.add_scalar("eval/avg_reward", avg_reward, epoch)
    writer.add_scalar("eval/avg_steps", avg_steps, epoch)
    writer.add_scalar("eval/reward_std", reward_std, epoch)
    writer.add_scalar("eval_raw/success_count", goal_count, epoch)
    writer.add_scalar("eval_raw/collision_count", collision_count, epoch)


def main(args=None):
    """ETHSRL+GPçš„ä¸»è¦è®­ç»ƒå¾ªç¯"""

    # ========== è®­ç»ƒé…ç½®ä¸è®¾å¤‡åˆå§‹åŒ– ==========
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    config = TrainingConfig()

    # ========== è®­ç»ƒåˆå§‹åŒ–æ—¥å¿— ==========
    print("\n" + "="*60)
    print("ğŸš€ Starting ETHSRL+GP Hierarchical Navigation Training")
    print("="*60)
    print(f"ğŸ“‹ Training Configuration:")
    print(f"   â€¢ Device: {device}")
    print(
        f"   â€¢ Max epochs: {config.max_epochs}, Episodes per epoch: {config.episodes_per_epoch}"
    )
    print(
        f"   â€¢ Training iterations: {config.training_iterations}, Batch size: {config.batch_size}"
    )
    print(f"   â€¢ Max steps per episode: {config.max_steps}")
    print(f"   â€¢ Train every {config.train_every_n_episodes} episodes")
    if config.save_every > 0:
        print(f"   â€¢ Save models every {config.save_every} episodes")
    else:
        print("   â€¢ Save models at end of training only")
    print("="*60)

    # ========== ç³»ç»Ÿåˆå§‹åŒ– ==========
    print("ğŸ”„ Initializing ETHSRL+GP system...")
    system = HierarchicalNavigationSystem(device=device, subgoal_threshold=config.subgoal_radius)
    replay_buffer = TD3ReplayAdapter(buffer_size=config.buffer_size)
    print("âœ… System initialization completed")

    # ========== ç¯å¢ƒåˆå§‹åŒ– ==========
    print("ğŸ”„ Initializing simulation environment...")
    sim = SIM(world_file="worlds/env_b_none.yaml", disable_plotting=False)
    print("âœ… Environment initialization completed")

    # ========== è®­ç»ƒç»Ÿè®¡å˜é‡åˆå§‹åŒ– ==========
    episode_reward = 0.0
    epoch_total_reward = 0.0
    epoch_total_steps = 0
    epoch_goal_count = 0
    epoch_collision_count = 0

    # è®­ç»ƒè®¡æ•°å™¨åˆå§‹åŒ–
    episode = 0
    epoch = 0

    print("\nğŸ¬ Starting main training loop...")
    print("-" * 50)

    # å¥–åŠ±é…ç½®åˆå§‹åŒ–
    low_reward_cfg = LowLevelRewardConfig()
    high_reward_cfg = HighLevelRewardConfig()
    high_level_buffer: List[Tuple[np.ndarray, np.ndarray, float, np.ndarray, float]] = []
    current_subgoal_context: Optional[SubgoalContext] = None

    # ========== ä¸»è®­ç»ƒå¾ªç¯ ==========
    while epoch < config.max_epochs:
        # é‡ç½®ç¯å¢ƒå’Œç³»ç»ŸçŠ¶æ€
        system.reset()
        current_subgoal_context = None
        system.current_subgoal = None

        latest_scan, distance, cos, sin, collision, goal, prev_action, _ = sim.reset()
        prev_action = [0.0, 0.0]  # é‡ç½®åŠ¨ä½œ
        current_subgoal_world: Optional[np.ndarray] = None

        steps = 0
        episode_reward = 0.0
        done = False

        # ========== å•æ¬¡æƒ…èŠ‚å¾ªç¯ ==========
        while not done and steps < config.max_steps:
            robot_pose = get_robot_pose(sim)
            goal_info = [distance, cos, sin]

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’å­ç›®æ ‡
            should_replan = (
                system.high_level_planner.current_subgoal_world is None
                or system.high_level_planner.check_triggers(
                    latest_scan,
                    robot_pose,
                    goal_info,
                    prev_action=prev_action,
                    current_step=steps,
                )
            )

            if should_replan:
                # å®Œæˆå½“å‰å­ç›®æ ‡å¹¶è®­ç»ƒ
                finalize_components = finalize_subgoal_transition(
                    current_subgoal_context,
                    high_level_buffer,
                    high_reward_cfg,
                    done=False,
                    reached_goal=False,
                    collision=False,
                    timed_out=False,
                )
                if finalize_components is not None:
                    metrics = maybe_train_high_level(
                        system.high_level_planner,
                        high_level_buffer,
                        config.batch_size,
                    )
                    if metrics:
                        # è®°å½•è®­ç»ƒæŒ‡æ ‡
                        for key, value in metrics.items():
                            system.high_level_planner.writer.add_scalar(
                                f"planner/{key}",
                                value,
                                system.high_level_planner.iter_count,
                            )

                # ç”Ÿæˆæ–°å­ç›®æ ‡
                subgoal_distance, subgoal_angle = system.high_level_planner.generate_subgoal(
                    latest_scan,
                    distance,
                    cos,
                    sin,
                    prev_action=prev_action,
                    robot_pose=robot_pose,
                    current_step=steps,
                )
                planner_world = system.high_level_planner.current_subgoal_world
                current_subgoal_world = np.asarray(planner_world, dtype=np.float32) if planner_world is not None else None
                system.high_level_planner.event_trigger.reset_time(steps)
                if current_subgoal_world is None:
                    current_subgoal_world = compute_subgoal_world(robot_pose, subgoal_distance, subgoal_angle)

                # æ„å»ºé«˜å±‚çŠ¶æ€å‘é‡
                start_state = system.high_level_planner.build_state_vector(
                    latest_scan,
                    distance,
                    cos,
                    sin,
                    prev_action,
                )

                # åˆ›å»ºæ–°çš„å­ç›®æ ‡ä¸Šä¸‹æ–‡
                current_subgoal_context = SubgoalContext(
                    start_state=start_state.astype(np.float32, copy=False),
                    action=np.array([subgoal_distance, subgoal_angle], dtype=np.float32),
                    world_target=current_subgoal_world,
                    start_goal_distance=distance,
                    last_goal_distance=distance,
                    low_level_return=0.0,
                    steps=0,
                    subgoal_completed=False,
                    last_state=start_state.astype(np.float32, copy=False),
                )
            else:
                planner_world = system.high_level_planner.current_subgoal_world
                if planner_world is not None:
                    current_subgoal_world = np.asarray(planner_world, dtype=np.float32)

            system.current_subgoal_world = current_subgoal_world

            relative_geometry = system.high_level_planner.get_relative_subgoal(robot_pose)
            if relative_geometry[0] is None:
                if should_replan and subgoal_distance is not None and subgoal_angle is not None:
                    relative_geometry = (subgoal_distance, subgoal_angle)
                elif system.current_subgoal is not None:
                    relative_geometry = system.current_subgoal
                else:
                    relative_geometry = (0.0, 0.0)

            subgoal_distance, subgoal_angle = float(relative_geometry[0]), float(relative_geometry[1])
            system.current_subgoal = (subgoal_distance, subgoal_angle)

            # è®¡ç®—å­ç›®æ ‡è·ç¦»
            prev_subgoal_distance = None
            if current_subgoal_world is not None:
                prev_pos = np.array(robot_pose[:2], dtype=np.float32)
                prev_subgoal_distance = float(np.linalg.norm(prev_pos - current_subgoal_world))

            # å¤„ç†ä½å±‚è§‚æµ‹
            state = system.low_level_controller.process_observation(
                latest_scan,
                subgoal_distance,
                subgoal_angle,
                prev_action,
            )

            # é¢„æµ‹åŠ¨ä½œï¼ˆå¸¦æ¢ç´¢å™ªå£°ï¼‰
            action = system.low_level_controller.predict_action(
                state,
                add_noise=True,
                noise_scale=config.exploration_noise,
            )
            action = np.clip(action, -1.0, 1.0)  # è£å‰ªåŠ¨ä½œ

            # è½¬æ¢ä¸ºå®é™…æ§åˆ¶å‘½ä»¤
            lin_cmd = float(np.clip((action[0] + 1.0) / 4.0, 0.0, config.max_lin_velocity))
            ang_cmd = float(np.clip(action[1], -config.max_ang_velocity, config.max_ang_velocity))

            # æ‰§è¡ŒåŠ¨ä½œ
            latest_scan, distance, cos, sin, collision, goal, executed_action, _ = sim.step(
                lin_velocity=lin_cmd,
                ang_velocity=ang_cmd,
            )

            # æ›´æ–°å­ç›®æ ‡è·ç¦»
            next_pose = get_robot_pose(sim)
            current_subgoal_distance = None
            if current_subgoal_world is not None:
                next_pos = np.array(next_pose[:2], dtype=np.float32)
                current_subgoal_distance = float(np.linalg.norm(next_pos - current_subgoal_world))

            # è®¡ç®—æœ€å°éšœç¢ç‰©è·ç¦»
            scan_arr = np.asarray(latest_scan, dtype=np.float32)
            finite_scan = scan_arr[np.isfinite(scan_arr)]
            min_obstacle_distance = float(finite_scan.min()) if finite_scan.size else 8.0

            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            just_reached_subgoal = False
            if (
                current_subgoal_distance is not None
                and current_subgoal_distance <= config.subgoal_radius
            ):
                if prev_subgoal_distance is None:
                    just_reached_subgoal = True
                elif prev_subgoal_distance > config.subgoal_radius:
                    just_reached_subgoal = True
            if (
                current_subgoal_context is not None
                and current_subgoal_context.subgoal_completed
            ):
                just_reached_subgoal = False
            timed_out = steps == config.max_steps - 1 and not (goal or collision)

            # è®¡ç®—ä½å±‚å¥–åŠ±
            low_reward, _ = compute_low_level_reward(
                prev_subgoal_distance=prev_subgoal_distance,
                current_subgoal_distance=current_subgoal_distance,
                min_obstacle_distance=min_obstacle_distance,
                reached_goal=goal,
                reached_subgoal=just_reached_subgoal,
                collision=collision,
                timed_out=timed_out,
                config=low_reward_cfg,
            )

            # æ›´æ–°å¥–åŠ±ç»Ÿè®¡
            episode_reward += low_reward
            epoch_total_reward += low_reward
            epoch_total_steps += 1

            # å®šæœŸè¾“å‡ºè®­ç»ƒè¿›åº¦
            if steps % 50 == 0:
                print(
                    f"ğŸƒ Training | Epoch {epoch:2d}/{config.max_epochs} | "
                    f"Episode {episode:3d}/{config.episodes_per_epoch} | "
                    f"Step {steps:3d}/{config.max_steps} | "
                    f"Reward: {low_reward:7.2f}"
                )

            # æ›´æ–°å­ç›®æ ‡ä¸Šä¸‹æ–‡
            if current_subgoal_context is not None:
                current_subgoal_context.low_level_return += low_reward
                current_subgoal_context.steps += 1
                current_subgoal_context.subgoal_completed |= just_reached_subgoal
                current_subgoal_context.last_goal_distance = distance
                # æ„å»ºä¸‹ä¸€çŠ¶æ€å‘é‡
                next_state_vector = system.high_level_planner.build_state_vector(
                    latest_scan,
                    distance,
                    cos,
                    sin,
                    executed_action,
                )
                current_subgoal_context.last_state = next_state_vector.astype(np.float32, copy=False)

            # å‡†å¤‡ä¸‹ä¸€çŠ¶æ€
            next_prev_action = [executed_action[0], executed_action[1]]
            next_state = system.low_level_controller.process_observation(
                latest_scan,
                system.current_subgoal[0] if system.current_subgoal else subgoal_distance,
                system.current_subgoal[1] if system.current_subgoal else subgoal_angle,
                next_prev_action,
            )

            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            done = collision or goal or steps == config.max_steps - 1

            # æ·»åŠ ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº
            replay_buffer.add(state, action, low_reward, float(done), next_state)

            prev_action = next_prev_action
            steps += 1

        # ========== æƒ…èŠ‚ç»“æŸå¤„ç† ==========
        timed_out_episode = not goal and not collision and steps >= config.max_steps

        # å®Œæˆæœ€åä¸€ä¸ªå­ç›®æ ‡
        finalize_components = finalize_subgoal_transition(
            current_subgoal_context,
            high_level_buffer,
            high_reward_cfg,
            done=True,
            reached_goal=goal,
            collision=collision,
            timed_out=timed_out_episode,
        )
        if finalize_components is not None:
            metrics = maybe_train_high_level(
                system.high_level_planner,
                high_level_buffer,
                config.batch_size,
            )
            if metrics:
                for key, value in metrics.items():
                    system.high_level_planner.writer.add_scalar(
                        f"planner/{key}",
                        value,
                        system.high_level_planner.iter_count,
                    )
        
        # é‡ç½®å­ç›®æ ‡ä¸Šä¸‹æ–‡
        current_subgoal_context = None
        current_subgoal_world = None

        # æ›´æ–°ç»Ÿè®¡
        if goal:
            epoch_goal_count += 1
        if collision:
            epoch_collision_count += 1

        # è¾“å‡ºæƒ…èŠ‚ç»“æœ
        status = "ğŸ¯ GOAL" if goal else "ğŸ’¥ COLLISION" if collision else "â° TIMEOUT"
        print(
            f"   Episode {episode:3d} finished: {status} | "
            f"Steps: {steps:3d} | Total Reward: {episode_reward:7.1f}"
        )

        # è®°å½•åˆ°TensorBoard
        writer = system.low_level_controller.writer
        writer.add_scalar("train/episode_reward", episode_reward, episode)

        # ========== è®­ç»ƒä½å±‚æ§åˆ¶å™¨ ==========
        if (
            replay_buffer.size() >= config.min_buffer_size
            and episode % config.train_every_n_episodes == 0
        ):
            current_buffer_size = replay_buffer.size()
            print(f"   ğŸ”„ Training model... (Buffer: {current_buffer_size} samples)")

            # æ‰§è¡Œå¤šæ¬¡è®­ç»ƒè¿­ä»£
            for _ in range(config.training_iterations):
                system.low_level_controller.update(
                    replay_buffer,
                    batch_size=config.batch_size,
                    discount=0.99,
                    tau=0.005,
                    policy_noise=0.2,
                    noise_clip=0.5,
                    policy_freq=2,
                )
            print("   âœ… Training completed")

        episode += 1

        # ========== æ¨¡å‹ä¿å­˜ ==========
        if config.save_every > 0 and episode % config.save_every == 0:
            print(f"   ğŸ’¾ Saving checkpoints after episode {episode}")
            system.high_level_planner.save_model(
                filename=system.high_level_planner.model_name,
                directory=system.high_level_planner.save_directory,
            )
            system.low_level_controller.save_model(
                filename=system.low_level_controller.model_name,
                directory=system.low_level_controller.save_directory,
            )

        # ========== è½®æ¬¡ç»“æŸå¤„ç† ==========
        if episode % config.episodes_per_epoch == 0:
            # è®¡ç®—è½®æ¬¡ç»Ÿè®¡
            epoch_avg_reward = epoch_total_reward / config.episodes_per_epoch
            epoch_success_rate = epoch_goal_count / config.episodes_per_epoch * 100
            epoch_collision_rate = epoch_collision_count / config.episodes_per_epoch * 100

            # è¾“å‡ºè½®æ¬¡æ€»ç»“
            print("\n" + "=" * 60)
            print(f"ğŸ“Š EPOCH {epoch:03d} TRAINING SUMMARY")
            print("=" * 60)
            print(
                f"   â€¢ Success Rate:    {epoch_success_rate:6.1f}% "
                f"({epoch_goal_count:2d}/{config.episodes_per_epoch:2d})"
            )
            print(
                f"   â€¢ Collision Rate:  {epoch_collision_rate:6.1f}% "
                f"({epoch_collision_count:2d}/{config.episodes_per_epoch:2d})"
            )
            print(f"   â€¢ Average Reward:  {epoch_avg_reward:8.2f}")
            print(f"   â€¢ Total Steps:     {epoch_total_steps:8d}")
            print(f"   â€¢ Buffer Size:     {replay_buffer.size():8d}")
            print("=" * 60)

            # é‡ç½®è½®æ¬¡ç»Ÿè®¡
            epoch_total_reward = 0.0
            epoch_total_steps = 0
            epoch_goal_count = 0
            epoch_collision_count = 0

            epoch += 1

            # æ‰§è¡Œè¯„ä¼°
            evaluate(system, sim, config, epoch, low_reward_cfg)

    # ========== è®­ç»ƒå®Œæˆå¤„ç† ==========
    print("\nğŸ’¾ Saving final checkpoints...")
    system.high_level_planner.save_model(
        filename=system.high_level_planner.model_name,
        directory=system.high_level_planner.save_directory,
    )
    system.low_level_controller.save_model(
        filename=system.low_level_controller.model_name,
        directory=system.low_level_controller.save_directory,
    )

    print("\n" + "="*60)
    print("ğŸ‰ ETHSRL+GP Training Completed!")
    print("="*60)
    print(f"ğŸ“ˆ Final performance after {config.max_epochs} epochs")
    print("="*60)


if __name__ == "__main__":
    main()
