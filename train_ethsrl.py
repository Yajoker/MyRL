"""
ETHSRL+GPåˆ†å±‚å¯¼èˆªç³»ç»Ÿçš„è®­ç»ƒå…¥å£ç‚¹

è¯¥è„šæœ¬éµå¾ªåŸå§‹``robot_nav/rl_train.py``çš„ç»“æ„ï¼Œ
åŒæ—¶é›†æˆäº†æ–°å®ç°çš„é«˜å±‚è§„åˆ’å™¨å’Œä½å±‚æ§åˆ¶å™¨ã€‚
"""

from dataclasses import dataclass
from typing import Tuple

import numpy as np
import torch

from ethsrl.core.integration import HierarchicalNavigationSystem
from robot_nav.SIM_ENV.sim import SIM
from robot_nav.replay_buffer import ReplayBuffer


@dataclass
class TrainingConfig:
    """è®­ç»ƒè¶…å‚æ•°é…ç½®å®¹å™¨"""

    buffer_size: int = 100_000  # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
    batch_size: int = 64  # è®­ç»ƒæ‰¹æ¬¡å¤§å°
    max_epochs: int = 60  # æœ€å¤§è®­ç»ƒè½®æ•°
    episodes_per_epoch: int = 70  # æ¯è½®è®­ç»ƒçš„æƒ…èŠ‚æ•°
    max_steps: int = 300  # æ¯ä¸ªæƒ…èŠ‚çš„æœ€å¤§æ­¥æ•°
    train_every_n_episodes: int = 2  # æ¯Nä¸ªæƒ…èŠ‚è®­ç»ƒä¸€æ¬¡
    training_iterations: int = 80  # æ¯æ¬¡è®­ç»ƒçš„è¿­ä»£æ¬¡æ•°
    exploration_noise: float = 0.2  # æ¢ç´¢å™ªå£°å¼ºåº¦
    min_buffer_size: int = 1_000  # å¼€å§‹è®­ç»ƒçš„æœ€å°ç¼“å†²åŒºå¤§å°
    max_lin_velocity: float = 0.5  # æœ€å¤§çº¿é€Ÿåº¦
    max_ang_velocity: float = 1.0  # æœ€å¤§è§’é€Ÿåº¦
    eval_episodes: int = 6  # è¯„ä¼°æ—¶ä½¿ç”¨çš„æƒ…èŠ‚æ•°


class TD3ReplayAdapter:
    """åŒ¹é…æ§åˆ¶å™¨æœŸæœ›çš„å›æ”¾ç¼“å†²åŒºAPIçš„è–„åŒ…è£…å™¨"""

    def __init__(self, buffer_size: int, random_seed: int = 666) -> None:
        """åˆå§‹åŒ–å›æ”¾ç¼“å†²åŒºé€‚é…å™¨"""
        self._buffer = ReplayBuffer(buffer_size=buffer_size, random_seed=random_seed)

    def add(self, state, action, reward, done, next_state) -> None:
        """å‘ç¼“å†²åŒºæ·»åŠ ç»éªŒ"""
        self._buffer.add(state, action, reward, done, next_state)

    def size(self) -> int:
        """è¿”å›ç¼“å†²åŒºå½“å‰å¤§å°"""
        return self._buffer.size()

    def sample(self, batch_size: int):
        """ä»ç¼“å†²åŒºé‡‡æ ·æ‰¹æ¬¡æ•°æ®"""
        states, actions, rewards, dones, next_states = self._buffer.sample_batch(batch_size)
        return states, actions, rewards, next_states, dones

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å†²åŒº"""
        self._buffer.clear()


def get_robot_pose(sim: SIM) -> Tuple[float, float, float]:
    """ä»IR-SimåŒ…è£…å™¨ä¸­æå–æœºå™¨äººä½å§¿å¹¶è¿”å›(x, y, theta)"""

    robot_state = sim.env.get_robot_state()  # è·å–æœºå™¨äººçŠ¶æ€
    return (
        float(robot_state[0].item()),  # xåæ ‡
        float(robot_state[1].item()),  # yåæ ‡
        float(robot_state[2].item()),  # èˆªå‘è§’theta
    )


def evaluate(system: HierarchicalNavigationSystem, sim: SIM, config: TrainingConfig, epoch: int) -> None:
    """è¿è¡Œæ— æ¢ç´¢å™ªå£°çš„è¯„ä¼° rollout å¹¶è®°å½•æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯"""

    print("\n" + "="*60)
    print(f"ğŸ¯ EPOCH {epoch:03d} EVALUATION")
    print("="*60)
    
    # åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡
    total_reward = 0.0
    total_steps = 0
    collision_count = 0
    goal_count = 0
    timeout_count = 0
    episode_rewards = []
    episode_lengths = []

    # è¿è¡Œå¤šä¸ªè¯„ä¼°å›åˆ
    for ep_idx in range(config.eval_episodes):
        system.reset()  # é‡ç½®ç³»ç»ŸçŠ¶æ€
        # é‡ç½®æ¨¡æ‹Ÿç¯å¢ƒå¹¶è·å–åˆå§‹è§‚æµ‹
        latest_scan, distance, cos, sin, collision, goal, prev_action, reward = sim.reset()
        prev_action = [0.0, 0.0]  # åˆå§‹åŒ–åŠ¨ä½œä¸ºé›¶
        done = False
        steps = 0
        episode_reward = 0.0

        # å•ä¸ªè¯„ä¼°å›åˆå¾ªç¯
        while not done and steps < config.max_steps:
            robot_pose = get_robot_pose(sim)  # è·å–æœºå™¨äººä½å§¿
            goal_info = [distance, cos, sin]  # ç›®æ ‡ä¿¡æ¯

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°å­ç›®æ ‡
            if system.current_subgoal is None or system.high_level_planner.check_triggers(
                latest_scan, robot_pose, goal_info
            ):
                # ç”Ÿæˆæ–°çš„å­ç›®æ ‡
                subgoal_distance, subgoal_angle = system.high_level_planner.generate_subgoal(
                    latest_scan, distance, cos, sin
                )
            else:
                # ä½¿ç”¨å½“å‰å­ç›®æ ‡
                subgoal_distance, subgoal_angle = system.current_subgoal

            # å¤„ç†è§‚æµ‹ä¸ºçŠ¶æ€å‘é‡
            state = system.low_level_controller.process_observation(
                latest_scan, subgoal_distance, subgoal_angle, prev_action
            )

            # é¢„æµ‹åŠ¨ä½œï¼ˆæ— æ¢ç´¢å™ªå£°ï¼‰
            action = system.low_level_controller.predict_action(state, add_noise=False)
            # å°†åŠ¨ä½œè½¬æ¢ä¸ºæ§åˆ¶å‘½ä»¤
            lin_cmd = float(np.clip((action[0] + 1.0) / 4.0, 0.0, config.max_lin_velocity))
            ang_cmd = float(np.clip(action[1], -config.max_ang_velocity, config.max_ang_velocity))

            # åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œ
            latest_scan, distance, cos, sin, collision, goal, _, reward = sim.step(
                lin_velocity=lin_cmd, ang_velocity=ang_cmd
            )

            prev_action = [lin_cmd, ang_cmd]  # æ›´æ–°å†å²åŠ¨ä½œ
            episode_reward += reward
            steps += 1

            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            if collision:
                collision_count += 1
                done = True
            elif goal:
                goal_count += 1
                done = True
            elif steps >= config.max_steps:
                timeout_count += 1
                done = True

        # è®°å½•å•å›åˆæ•°æ®
        episode_rewards.append(episode_reward)
        episode_lengths.append(steps)
        total_reward += episode_reward
        total_steps += steps
        
        # æ˜¾ç¤ºå•å›åˆè¯„ä¼°è¿›åº¦
        status = "ğŸ¯" if goal else "ğŸ’¥" if collision else "â°"
        print(f"   Evaluation Episode {ep_idx+1:2d}/{config.eval_episodes}: {status} | "
              f"Steps: {steps:3d} | Reward: {episode_reward:7.1f}")

    # ========== è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡ ==========
    avg_reward = total_reward / config.eval_episodes
    avg_steps = total_steps / config.eval_episodes
    success_rate = goal_count / config.eval_episodes * 100
    collision_rate = collision_count / config.eval_episodes * 100
    timeout_rate = timeout_count / config.eval_episodes * 100
    
    # è®¡ç®—æ ‡å‡†å·®
    reward_std = np.std(episode_rewards) if config.eval_episodes > 1 else 0
    steps_std = np.std(episode_lengths) if config.eval_episodes > 1 else 0
    
    # ========== æ ¼å¼åŒ–è¾“å‡º ==========
    print("\nğŸ“ˆ Performance Summary:")
    print(f"   â€¢ Success Rate:      {success_rate:6.1f}% ({goal_count:2d}/{config.eval_episodes:2d})")
    print(f"   â€¢ Collision Rate:    {collision_rate:6.1f}% ({collision_count:2d}/{config.eval_episodes:2d})")
    print(f"   â€¢ Timeout Rate:      {timeout_rate:6.1f}% ({timeout_count:2d}/{config.eval_episodes:2d})")
    print(f"   â€¢ Average Reward:    {avg_reward:8.2f} Â± {reward_std:.2f}")
    print(f"   â€¢ Average Steps:     {avg_steps:8.1f} Â± {steps_std:.1f}")
    
    # é¢å¤–æŒ‡æ ‡
    if goal_count > 0:
        successful_episodes_reward = sum(r for i, r in enumerate(episode_rewards) 
                                       if episode_lengths[i] < config.max_steps and not collision)
        avg_success_reward = successful_episodes_reward / goal_count
        print(f"   â€¢ Avg Success Reward: {avg_success_reward:8.2f}")
    
    print("-" * 60)
    print(f"â° Evaluation completed: {config.eval_episodes} episodes")
    print("=" * 60)
    
    # ========== TensorBoardè®°å½• ==========
    writer = system.low_level_controller.writer
    writer.add_scalar("eval/success_rate", success_rate, epoch)
    writer.add_scalar("eval/collision_rate", collision_rate, epoch)
    writer.add_scalar("eval/timeout_rate", timeout_rate, epoch)
    writer.add_scalar("eval/avg_reward", avg_reward, epoch)
    writer.add_scalar("eval/avg_steps", avg_steps, epoch)
    writer.add_scalar("eval/reward_std", reward_std, epoch)
    
    # è®°å½•åŸå§‹è®¡æ•°
    writer.add_scalar("eval_raw/success_count", goal_count, epoch)
    writer.add_scalar("eval_raw/collision_count", collision_count, epoch)


def main(args=None):
    """ETHSRL+GPçš„ä¸»è¦è®­ç»ƒå¾ªç¯"""

    # ========== è®­ç»ƒåˆå§‹åŒ–æ—¥å¿— ==========
    print("\n" + "="*60)
    print("ğŸš€ Starting ETHSRL+GP Hierarchical Navigation Training")
    print("="*60)
    print(f"ğŸ“‹ Training Configuration:")
    print(f"   â€¢ Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}")
    print(f"   â€¢ Max epochs: {60}, Episodes per epoch: {70}")
    print(f"   â€¢ Training iterations: {80}, Batch size: {64}")
    print(f"   â€¢ Max steps per episode: {300}")
    print(f"   â€¢ Train every {2} episodes")
    print("="*60)

    # è®¾å¤‡å’Œç³»ç»Ÿåˆå§‹åŒ–
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    config = TrainingConfig()

    # ========== ç³»ç»Ÿåˆå§‹åŒ– ==========
    print("ğŸ”„ Initializing ETHSRL+GP system...")
    system = HierarchicalNavigationSystem(device=device)
    replay_buffer = TD3ReplayAdapter(buffer_size=config.buffer_size)
    print("âœ… System initialization completed")

    # ========== ç¯å¢ƒåˆå§‹åŒ– ==========
    print("ğŸ”„ Initializing simulation environment...")
    sim = SIM(world_file="worlds/env_a.yaml", disable_plotting=False)
    print("âœ… Environment initialization completed")

    # ========== è®­ç»ƒç»Ÿè®¡å˜é‡åˆå§‹åŒ– ==========
    episode_reward = 0.0
    epoch_total_reward = 0.0
    epoch_total_steps = 0
    epoch_goal_count = 0
    epoch_collision_count = 0

    # è®­ç»ƒè®¡æ•°å™¨åˆå§‹åŒ–
    episode = 0
    epoch = 0

    print("\nğŸ¬ Starting main training loop...")
    print("-" * 50)

    # ä¸»è®­ç»ƒå¾ªç¯
    while epoch < config.max_epochs:
        system.reset()
        # é‡ç½®æ¨¡æ‹Ÿç¯å¢ƒå¹¶è·å–åˆå§‹è§‚æµ‹
        latest_scan, distance, cos, sin, collision, goal, prev_action, reward = sim.reset()
        prev_action = [0.0, 0.0]

        # å•ä¸ªæƒ…èŠ‚å˜é‡åˆå§‹åŒ–
        steps = 0
        episode_reward = 0.0
        done = False

        # å•ä¸ªæƒ…èŠ‚å¾ªç¯
        while not done and steps < config.max_steps:
            robot_pose = get_robot_pose(sim)
            goal_info = [distance, cos, sin]

            # ç¡®å®šæ˜¯å¦éœ€è¦æ–°çš„å­ç›®æ ‡
            if system.current_subgoal is None or system.high_level_planner.check_triggers(
                latest_scan, robot_pose, goal_info
            ):
                subgoal_distance, subgoal_angle = system.high_level_planner.generate_subgoal(
                    latest_scan, distance, cos, sin
                )
            else:
                subgoal_distance, subgoal_angle = system.current_subgoal

            # å¤„ç†è§‚æµ‹ä¸ºçŠ¶æ€å‘é‡
            state = system.low_level_controller.process_observation(
                latest_scan, subgoal_distance, subgoal_angle, prev_action
            )

            # é¢„æµ‹åŠ¨ä½œï¼ˆå¸¦æ¢ç´¢å™ªå£°ï¼‰
            action = system.low_level_controller.predict_action(
                state, add_noise=True, noise_scale=config.exploration_noise
            )
            action = np.clip(action, -1.0, 1.0)

            # å°†åŠ¨ä½œè½¬æ¢ä¸ºæ§åˆ¶å‘½ä»¤
            lin_cmd = float(np.clip((action[0] + 1.0) / 4.0, 0.0, config.max_lin_velocity))
            ang_cmd = float(np.clip(action[1], -config.max_ang_velocity, config.max_ang_velocity))

            # åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œ
            latest_scan, distance, cos, sin, collision, goal, executed_action, reward = sim.step(
                lin_velocity=lin_cmd, ang_velocity=ang_cmd
            )

            episode_reward += reward
            epoch_total_reward += reward
            epoch_total_steps += 1

            # æ¯50æ­¥æ˜¾ç¤ºä¸€æ¬¡è®­ç»ƒè¿›åº¦
            if steps % 50 == 0:
                print(f"ğŸƒ Training | Epoch {epoch:2d}/{config.max_epochs} | "
                      f"Episode {episode:3d}/{config.episodes_per_epoch} | "
                      f"Step {steps:3d}/{config.max_steps} | "
                      f"Reward: {reward:7.2f}")

            # å‡†å¤‡ä¸‹ä¸€ä¸ªçŠ¶æ€
            next_prev_action = [executed_action[0], executed_action[1]]
            next_state = system.low_level_controller.process_observation(
                latest_scan,
                system.current_subgoal[0] if system.current_subgoal else subgoal_distance,
                system.current_subgoal[1] if system.current_subgoal else subgoal_angle,
                next_prev_action,
            )

            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            done = collision or goal or steps == config.max_steps - 1
            
            # å°†ç»éªŒæ·»åŠ åˆ°å›æ”¾ç¼“å†²åŒº
            replay_buffer.add(state, action, reward, float(done), next_state)

            prev_action = next_prev_action
            steps += 1

        # æ›´æ–°ç»Ÿè®¡
        if goal:
            epoch_goal_count += 1
        if collision:
            epoch_collision_count += 1

        # æ˜¾ç¤ºå›åˆç»“æŸä¿¡æ¯
        status = "ğŸ¯ GOAL" if goal else "ğŸ’¥ COLLISION" if collision else "â° TIMEOUT"
        print(f"   Episode {episode:3d} finished: {status} | "
              f"Steps: {steps:3d} | Total Reward: {episode_reward:7.1f}")

        # è®°å½•æƒ…èŠ‚å¥–åŠ±åˆ°TensorBoard
        writer = system.low_level_controller.writer
        writer.add_scalar("train/episode_reward", episode_reward, episode)

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿›è¡Œè®­ç»ƒ
        if (
            replay_buffer.size() >= config.min_buffer_size
            and episode % config.train_every_n_episodes == 0
        ):
            current_buffer_size = replay_buffer.size()
            print(f"   ğŸ”„ Training model... (Buffer: {current_buffer_size} samples)")
            
            # æ‰§è¡Œè®­ç»ƒè¿­ä»£
            for _ in range(config.training_iterations):
                system.low_level_controller.update(
                    replay_buffer,
                    batch_size=config.batch_size,
                    discount=0.99,
                    tau=0.005,
                    policy_noise=0.2,
                    noise_clip=0.5,
                    policy_freq=2,
                )
            print(f"   âœ… Training completed")

        episode += 1

        # æ£€æŸ¥æ˜¯å¦å®Œæˆä¸€ä¸ªè®­ç»ƒè½®æ¬¡
        if episode % config.episodes_per_epoch == 0:
            # è®­ç»ƒé˜¶æ®µç»Ÿè®¡
            epoch_avg_reward = epoch_total_reward / config.episodes_per_epoch
            epoch_success_rate = epoch_goal_count / config.episodes_per_epoch * 100
            epoch_collision_rate = epoch_collision_count / config.episodes_per_epoch * 100
            
            print("\n" + "="*60)
            print(f"ğŸ“Š EPOCH {epoch:03d} TRAINING SUMMARY")
            print("="*60)
            print(f"   â€¢ Success Rate:    {epoch_success_rate:6.1f}% ({epoch_goal_count:2d}/{config.episodes_per_epoch:2d})")
            print(f"   â€¢ Collision Rate:  {epoch_collision_rate:6.1f}% ({epoch_collision_count:2d}/{config.episodes_per_epoch:2d})")
            print(f"   â€¢ Average Reward:  {epoch_avg_reward:8.2f}")
            print(f"   â€¢ Total Steps:     {epoch_total_steps:8d}")
            print(f"   â€¢ Buffer Size:     {replay_buffer.size():8d}")
            print("="*60)
            
            # é‡ç½®epochç»Ÿè®¡
            epoch_total_reward = 0.0
            epoch_total_steps = 0
            epoch_goal_count = 0
            epoch_collision_count = 0
            
            epoch += 1
            
            # è¿è¡Œè¯„ä¼°
            evaluate(system, sim, config, epoch)

    # ========== è®­ç»ƒå®Œæˆæ—¥å¿— ==========
    print("\n" + "="*60)
    print("ğŸ‰ ETHSRL+GP Training Completed!")
    print("="*60)
    print(f"ğŸ“ˆ Final performance after {config.max_epochs} epochs")
    print("="*60)


if __name__ == "__main__":
    main()
